{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train\n",
    "\n",
    "**Algorithms**:\n",
    "\n",
    "**K-Nearest Neighbors (KNN):**\n",
    "- *Pros:* Intuitive, no complex training required.\n",
    "- *Cons:* Performance may degrade with large datasets, sensitive to feature scale.\n",
    "\n",
    "**Decision Trees:**\n",
    "- *Pros:* Intuitive, automatically handles relevant features.\n",
    "- *Cons:* Tendency to overfit, can be unstable with small variations in data.\n",
    "\n",
    "**Linear Regression:**\n",
    "- *Pros:* Simple and interpretable, suitable for capturing linear relationships.\n",
    "- *Cons:* Sensitive to outliers, may not handle complex stock price movements well.\n",
    "\n",
    "**Logistic Regression:**\n",
    "- *Pros:* Suitable for binary classification tasks.\n",
    "- *Cons:* Assumes linear relationship, may struggle with capturing intricate stock price patterns.\n",
    "\n",
    "**Support Vector Machines (SVM):**\n",
    "- *Pros:* Good in high-dimensional spaces.\n",
    "- *Cons:* Requires careful parameter tuning, may not be optimal for large datasets.\n",
    "\n",
    "**Random Forest:**\n",
    "- *Pros:* Effective at handling complexity, provides feature importance.\n",
    "- *Cons:* Less interpretable, training can be time-consuming.\n",
    "\n",
    "**Ensemble Methods:**\n",
    "- *Pros:* Combines models for improved performance.\n",
    "- *Cons:* Complexity and interpretability challenges.\n",
    "\n",
    "**Neural Networks:**\n",
    "- *Pros:* Effective for complex, non-linear patterns.\n",
    "- *Cons:* Requires substantial data and computational resources, architecture complexity.\n",
    "\n",
    "We exclude Linear Regression, Decision Trees, Support Vector Machines (SVM), and Convolutional Neural Networks (CNN).\n",
    "\n",
    "**Considerations**:\n",
    "\n",
    "We can use the remaining algorithms to split the problem into three categories: Target 1 day, Target 5 days, Target 30 days. If the prediction doesn't belong to any of these categories, we classify the problem as Unsafe.\n",
    "\n",
    "We then train the dataset to find the best algorithm with the following algorithms:\n",
    "- Knn\n",
    "- Logistic Regression \n",
    "- Random Forests\n",
    "- Ensemble Methods: \n",
    "- Neural Networks:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28918e1ed52316e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc15b3fd4a850708"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_blobs\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T11:11:58.768707Z",
     "start_time": "2023-12-14T11:11:58.046155Z"
    }
   },
   "id": "45384eb0b2c9c55a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "        Date        Open        High         Low       Close   Volume  \\\n0 2020-06-30  179.305945  183.533295  179.102439  182.802521  3102800   \n1 2020-07-01  183.968061  184.763579  180.859992  182.756287  2620100   \n2 2020-07-02  187.316608  187.779118  182.349254  182.598999  2699400   \n3 2020-07-06  186.243599  192.209977  186.049352  191.812225  3567700   \n4 2020-07-07  190.091704  190.285964  184.254827  184.412079  2853500   \n\n   Dividends  Stock Splits  Daily_Return  Daily_Return_Percentage  ...  \\\n0        0.0           0.0     -3.496576                -6.269568  ...   \n1        0.0           0.0      1.211774                 2.229278  ...   \n2        0.0           0.0      4.717609                 8.836865  ...   \n3        0.0           0.0     -5.568627               -10.371211  ...   \n4        0.0           0.0      5.679625                10.796496  ...   \n\n        MA_30       MA_50        RSI      MACD  Signal_Line  \\\n0  185.971379  177.444586  40.980241  0.824023     3.006285   \n1  186.614105  177.904132  52.500011  0.573091     2.519646   \n2  187.140969  178.320634  46.428544  0.357413     2.087199   \n3  188.016001  178.938500  50.786526  0.919320     1.853623   \n4  188.649571  179.372512  42.843182  0.758759     1.634651   \n\n   Bollinger_Mid_Band  Bollinger_Upper_Band  Bollinger_Lower_Band  Volatility  \\\n0          190.221655            205.998882            174.444429    6.840469   \n1          189.620391            205.574176            173.666606    6.521267   \n2          188.814696            204.471975            173.157416    5.091692   \n3          188.326285            202.922613            173.729956    4.048404   \n4          187.334203            200.017990            174.650416    4.947823   \n\n   Ticker  \n0      GS  \n1      GS  \n2      GS  \n3      GS  \n4      GS  \n\n[5 rows x 44 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>Dividends</th>\n      <th>Stock Splits</th>\n      <th>Daily_Return</th>\n      <th>Daily_Return_Percentage</th>\n      <th>...</th>\n      <th>MA_30</th>\n      <th>MA_50</th>\n      <th>RSI</th>\n      <th>MACD</th>\n      <th>Signal_Line</th>\n      <th>Bollinger_Mid_Band</th>\n      <th>Bollinger_Upper_Band</th>\n      <th>Bollinger_Lower_Band</th>\n      <th>Volatility</th>\n      <th>Ticker</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-06-30</td>\n      <td>179.305945</td>\n      <td>183.533295</td>\n      <td>179.102439</td>\n      <td>182.802521</td>\n      <td>3102800</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-3.496576</td>\n      <td>-6.269568</td>\n      <td>...</td>\n      <td>185.971379</td>\n      <td>177.444586</td>\n      <td>40.980241</td>\n      <td>0.824023</td>\n      <td>3.006285</td>\n      <td>190.221655</td>\n      <td>205.998882</td>\n      <td>174.444429</td>\n      <td>6.840469</td>\n      <td>GS</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-07-01</td>\n      <td>183.968061</td>\n      <td>184.763579</td>\n      <td>180.859992</td>\n      <td>182.756287</td>\n      <td>2620100</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.211774</td>\n      <td>2.229278</td>\n      <td>...</td>\n      <td>186.614105</td>\n      <td>177.904132</td>\n      <td>52.500011</td>\n      <td>0.573091</td>\n      <td>2.519646</td>\n      <td>189.620391</td>\n      <td>205.574176</td>\n      <td>173.666606</td>\n      <td>6.521267</td>\n      <td>GS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-07-02</td>\n      <td>187.316608</td>\n      <td>187.779118</td>\n      <td>182.349254</td>\n      <td>182.598999</td>\n      <td>2699400</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.717609</td>\n      <td>8.836865</td>\n      <td>...</td>\n      <td>187.140969</td>\n      <td>178.320634</td>\n      <td>46.428544</td>\n      <td>0.357413</td>\n      <td>2.087199</td>\n      <td>188.814696</td>\n      <td>204.471975</td>\n      <td>173.157416</td>\n      <td>5.091692</td>\n      <td>GS</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-07-06</td>\n      <td>186.243599</td>\n      <td>192.209977</td>\n      <td>186.049352</td>\n      <td>191.812225</td>\n      <td>3567700</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-5.568627</td>\n      <td>-10.371211</td>\n      <td>...</td>\n      <td>188.016001</td>\n      <td>178.938500</td>\n      <td>50.786526</td>\n      <td>0.919320</td>\n      <td>1.853623</td>\n      <td>188.326285</td>\n      <td>202.922613</td>\n      <td>173.729956</td>\n      <td>4.048404</td>\n      <td>GS</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-07-07</td>\n      <td>190.091704</td>\n      <td>190.285964</td>\n      <td>184.254827</td>\n      <td>184.412079</td>\n      <td>2853500</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.679625</td>\n      <td>10.796496</td>\n      <td>...</td>\n      <td>188.649571</td>\n      <td>179.372512</td>\n      <td>42.843182</td>\n      <td>0.758759</td>\n      <td>1.634651</td>\n      <td>187.334203</td>\n      <td>200.017990</td>\n      <td>174.650416</td>\n      <td>4.947823</td>\n      <td>GS</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 44 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.ExcelFile('final_dataset.xlsx').parse('Sheet1')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T11:12:08.740218Z",
     "start_time": "2023-12-14T11:11:58.769100Z"
    }
   },
   "id": "956e36475c7e88af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Let's transform the following features into float-type data. This transformation is essential to ensure that the data can be processed by our training algorithms effectively. Converting these features to float allows our algorithms to handle and analyze the data appropriately during the training process. This step is crucial for the accuracy and efficiency of the machine learning models we'll be using."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f99330baae5213e0"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# drop rows that has -1 in target 1,5,30 days\n",
    "df = df[(df['Target_1day'] != -1) & (df['Target_5days'] != -1) & (df['Target_30days'] != -1)]\n",
    "df['Date'] = pd.to_datetime(df['Date']).astype(int) / 10**9\n",
    "df['Ticker'] = pd.factorize(df.Ticker)[0]\n",
    "df['Volume'] = df['Volume'].astype(float)\n",
    "df['Target_1day'] = df['Target_1day'].astype(float)\n",
    "df['Target_5days'] = df['Target_5days'].astype(float)\n",
    "df['Target_30days'] = df['Target_30days'].astype(float)\n",
    "df['Net Income'] = df['Net Income'].astype(float)\n",
    "df['Total Revenue'] = df['Total Revenue'].astype(float)\n",
    "df['Normalized EBITDA'] = df['Normalized EBITDA'].astype(float)\n",
    "df['Total Unusual Items'] = df['Total Unusual Items'].astype(float)\n",
    "df['Total Unusual Items Excluding Goodwill'] = df['Total Unusual Items Excluding Goodwill'].astype(float)\n",
    "df['Operating Cash Flow'] = df['Operating Cash Flow'].astype(float)\n",
    "df['Capital Expenditure'] = df['Capital Expenditure'].astype(float)\n",
    "df['Free Cash Flow'] = df['Free Cash Flow'].astype(float)\n",
    "df['Cash Flow From Continuing Operating Activities'] = df['Cash Flow From Continuing Operating Activities'].astype(float)\n",
    "df['Cash Flow From Continuing Investing Activities'] = df['Cash Flow From Continuing Investing Activities'].astype(float)\n",
    "df['Cash Flow From Continuing Financing Activities'] = df['Cash Flow From Continuing Financing Activities'].astype(float)\n",
    "df['Ticker'] = df['Ticker'].astype(float)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T11:12:08.910990Z",
     "start_time": "2023-12-14T11:12:08.741826Z"
    }
   },
   "id": "7394df20cac7aa62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train, Validation, Test \n",
    "\n",
    "The dataset `df` is divided into features (`X`) and three different target variables (`Y_1`, `Y_2`, and `Y_3`), corresponding to predicting stock values for 1, 5, and 30 days, respectively. The data is then split into training (80%), validation (20%), and test sets (20%). This separation ensures that the machine learning models can be trained, validated, and tested on distinct subsets of the data, facilitating the evaluation of their performance on different time horizons."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f86f710f01d832a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "X = df.drop(['Target_1day', 'Target_5days', 'Target_30days'], axis=1)\n",
    "Y_1 = df['Target_1day']\n",
    "Y_2 = df['Target_5days']\n",
    "Y_3 = df['Target_30days']\n",
    "\n",
    "X_train_1_80, X_test_1, Y_train_1_80, Y_test_1 = train_test_split(X, Y_1, test_size=0.2)\n",
    "X_train_1, X_valid_1, Y_train_1, Y_valid_1 = train_test_split(X_train_1_80, Y_train_1_80, test_size=0.20)\n",
    "\n",
    "\n",
    "X_train_2_80, X_test_2, Y_train_2_80, Y_test_2 = train_test_split(X, Y_2, test_size=0.2)\n",
    "X_valid_2, X_train_2, Y_valid_2, Y_train_2 = train_test_split(X_train_2_80, Y_train_2_80, test_size=0.20)\n",
    "\n",
    "X_train_3_80, X_test_3, Y_train_3_80, Y_test_3 = train_test_split(X, Y_3, test_size=0.2)\n",
    "X_valid_3, X_train_3, Y_valid_3, Y_train_3 = train_test_split(X_train_3_80, Y_train_3_80, test_size=0.20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T11:12:09.397590Z",
     "start_time": "2023-12-14T11:12:08.844997Z"
    }
   },
   "id": "e3407a2502373199"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the subsequent section of the document, we will be conducting testing on the selected machine learning algorithms to assess their performance in predicting stock values. The focus will be on evaluating the algorithms based on accuracy to determine which one is most effective for solving this specific problem. This testing phase aims to provide insights into the algorithm that yields the most accurate predictions for the given dataset and target variables."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbe8fe2d4d40732f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Knn\n",
    "\n",
    "In this section, we explore the K-Nearest Neighbors (KNN) algorithm by varying the parameter K, which represents the number of neighbors considered for classification. The objective is to identify the optimal K value that produces the most accurate predictions for our specific stock value prediction problem. By systematically testing different K values, we aim to determine the configuration that yields the highest accuracy, providing valuable insights into the performance of the KNN algorithm in this context."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9c62a335a471617"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 5\n",
      "Train set 1: 0.70\n",
      "Validation set 1: 0.52\n",
      "Train set 2: 0.73\n",
      "Validation set 2: 0.56\n",
      "Train set 3: 0.83\n",
      "Validation set 3: 0.74\n",
      "\n",
      "\n",
      "K: 10\n",
      "Train set 1: 0.64\n",
      "Validation set 1: 0.52\n",
      "Train set 2: 0.65\n",
      "Validation set 2: 0.54\n",
      "Train set 3: 0.75\n",
      "Validation set 3: 0.67\n",
      "\n",
      "\n",
      "K: 15\n",
      "Train set 1: 0.61\n",
      "Validation set 1: 0.51\n",
      "Train set 2: 0.63\n",
      "Validation set 2: 0.54\n",
      "Train set 3: 0.71\n",
      "Validation set 3: 0.65\n",
      "\n",
      "\n",
      "K: 20\n",
      "Train set 1: 0.60\n",
      "Validation set 1: 0.51\n",
      "Train set 2: 0.61\n",
      "Validation set 2: 0.54\n",
      "Train set 3: 0.69\n",
      "Validation set 3: 0.63\n",
      "\n",
      "\n",
      "K: 25\n",
      "Train set 1: 0.59\n",
      "Validation set 1: 0.52\n",
      "Train set 2: 0.60\n",
      "Validation set 2: 0.53\n",
      "Train set 3: 0.66\n",
      "Validation set 3: 0.63\n",
      "\n",
      "\n",
      "Best K: 5\n",
      "Test set 1: 0.52\n",
      "Test set 2: 0.66\n",
      "Test set 3: 0.84\n",
      "Total acc: 0.68\n"
     ]
    }
   ],
   "source": [
    "best_k = []\n",
    "for i in [5,10,15,20,25]:\n",
    "    print('K: ' + str(i) + '\\n')\n",
    "    # Target 1 day\n",
    "    knn_1 = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn_1.fit(X_train_1, Y_train_1)\n",
    "    train_acc_1 = accuracy_score(y_true= Y_train_1, y_pred= knn_1.predict(X_train_1))\n",
    "    valid_acc_1 = accuracy_score(y_true= Y_valid_1, y_pred= knn_1.predict(X_valid_1))\n",
    "    print(\"Train set 1: {:.2f}\".format(train_acc_1))\n",
    "    print('Validation set 1: {:.2f}'.format(valid_acc_1))\n",
    "    # Target 5 days\n",
    "    knn_2 = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn_2.fit(X_train_2, Y_train_2)\n",
    "    train_acc_2 = accuracy_score(y_true= Y_train_2, y_pred= knn_2.predict(X_train_2))\n",
    "    valid_acc_2 = accuracy_score(y_true= Y_valid_2, y_pred= knn_2.predict(X_valid_2))\n",
    "    print(\"Train set 2: {:.2f}\".format(train_acc_2))\n",
    "    print('Validation set 2: {:.2f}'.format(valid_acc_2))\n",
    "    # Target 30 days\n",
    "    knn_3 = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn_3.fit(X_train_3, Y_train_3)\n",
    "    train_acc_3 = accuracy_score(y_true= Y_train_3, y_pred= knn_3.predict(X_train_3))\n",
    "    valid_acc_3 = accuracy_score(y_true= Y_valid_3, y_pred= knn_3.predict(X_valid_3))\n",
    "    print(\"Train set 3: {:.2f}\".format(train_acc_3))\n",
    "    print('Validation set 3: {:.2f}'.format(valid_acc_3))\n",
    "    print('\\n')\n",
    "    best_k.append([i, (valid_acc_1 + valid_acc_2 + valid_acc_3) / 3])\n",
    "    \n",
    "k = max(best_k, key=lambda x:x[1])[0]\n",
    "print('Best K: ' + str(k) + '\\n')\n",
    "# Target 1 day\n",
    "knn_1 = KNeighborsClassifier(n_neighbors=k)\n",
    "knn_1.fit(X_train_1_80, Y_train_1_80)\n",
    "test_acc_1 = accuracy_score(y_true= Y_test_1, y_pred= knn_1.predict(X_test_1))\n",
    "print('Test set 1: {:.2f}'.format(test_acc_1))\n",
    "\n",
    "# Target 5 days\n",
    "knn_2 = KNeighborsClassifier(n_neighbors=k)\n",
    "knn_2.fit(X_train_2_80, Y_train_2_80)\n",
    "test_acc_2 = accuracy_score(y_true= Y_test_2, y_pred= knn_2.predict(X_test_2))\n",
    "print('Test set 2: {:.2f}'.format(test_acc_2))\n",
    "\n",
    "# Target 30 days\n",
    "knn_3 = KNeighborsClassifier(n_neighbors=k)\n",
    "knn_3.fit(X_train_3_80, Y_train_3_80)\n",
    "test_acc_3 = accuracy_score(y_true= Y_test_3, y_pred= knn_3.predict(X_test_3))\n",
    "print('Test set 3: {:.2f}'.format(test_acc_3))\n",
    "\n",
    "# Total acc\n",
    "print('Total acc: {:.2f}'.format((test_acc_1 + test_acc_2 + test_acc_3) / 3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T11:12:30.027459Z",
     "start_time": "2023-12-14T11:12:08.918566Z"
    }
   },
   "id": "2e260f77946702c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are some important considerations to take into account:\n",
    "\n",
    "1. **Small \\(k\\):** A small \\(k\\) value implies that the prediction for a data point is heavily influenced by its immediate neighbors. This makes the model sensitive to local variations in the training data, which might not generalize well to unseen data.\n",
    "\n",
    "2. **Performance on Test Sets:**\n",
    "   - **Target 1 (1 day):** The model achieved an accuracy of 51%, suggesting a relatively weak predictive performance for the next day.\n",
    "   - **Target 2 (5 days):** The accuracy increased to 66%, indicating a better performance in predicting the stock values over a 5-day period.\n",
    "   - **Target 3 (30 days):** The model exhibited the highest accuracy of 85%, suggesting a better ability to predict stock values over a longer timeframe.\n",
    "\n",
    "3. **Total Accuracy:**\n",
    "   - The total accuracy across all targets is reported as 67%, which is the average of the accuracies for the three target variables.\n",
    "\n",
    "4. **Generalization Concerns:** While the model might perform well on the training and validation sets, the real test lies in its ability to generalize to unseen data. The model's performance on the test sets should be carefully examined to assess its effectiveness in predicting stock values for different time horizons.\n",
    "\n",
    "In conclusion, the current model with \\(k=5\\) exhibits varying performance across different target variables. The trade-off between bias and variance needs to be carefully managed, and further exploration of hyperparameter values is recommended for a more robust and reliable KNN model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c87f21d30247135a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this section, we explore the training process of Logistic Regression for the stock prediction problem, aiming to analyze its outcomes. Logistic Regression is a well-established algorithm for binary classification tasks, making it suitable for predicting whether stock values will rise or fall.\n",
    "\n",
    "First of all we scale the data to improve the performance of the algorithm:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7c7e8e97b2f33dd"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_1_80_scaled = scaler.fit_transform(X_train_1_80)\n",
    "X_valid_1_scaled = scaler.transform(X_valid_1)\n",
    "X_test_1_scaled = scaler.transform(X_test_1)\n",
    "X_train_1_scaled = scaler.transform(X_train_1)\n",
    "\n",
    "X_train_2_80_scaled = scaler.fit_transform(X_train_2_80)\n",
    "X_valid_2_scaled = scaler.transform(X_valid_2)\n",
    "X_test_2_scaled = scaler.transform(X_test_2)\n",
    "X_train_2_scaled = scaler.transform(X_train_2)\n",
    "\n",
    "X_train_3_80_scaled = scaler.fit_transform(X_train_3_80)\n",
    "X_valid_3_scaled = scaler.transform(X_valid_3)\n",
    "X_test_3_scaled = scaler.transform(X_test_3)\n",
    "X_train_3_scaled = scaler.transform(X_train_3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T11:12:30.100205Z",
     "start_time": "2023-12-14T11:12:30.024936Z"
    }
   },
   "id": "945e0681dc03d791"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set 1: 0.51\n",
      "Validation set 1: 0.50\n",
      "Train set 2: 0.54\n",
      "Validation set 2: 0.52\n",
      "Train set 3: 0.56\n",
      "Validation set 3: 0.56\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# Target 1 day\n",
    "lr_1 = LogisticRegression(max_iter=1000)\n",
    "lr_1.fit(X_train_1_scaled, Y_train_1)\n",
    "train_acc_1 = accuracy_score(y_true= Y_train_1, y_pred= lr_1.predict(X_train_1_scaled))\n",
    "valid_acc_1 = accuracy_score(y_true= Y_valid_1, y_pred= lr_1.predict(X_valid_1_scaled))\n",
    "print(\"Train set 1: {:.2f}\".format(train_acc_1))\n",
    "print('Validation set 1: {:.2f}'.format(valid_acc_1))\n",
    "# Target 5 days\n",
    "lr_2 = LogisticRegression(max_iter=1000)\n",
    "lr_2.fit(X_train_2_scaled, Y_train_2)\n",
    "train_acc_2 = accuracy_score(y_true= Y_train_2, y_pred= lr_2.predict(X_train_2_scaled))\n",
    "valid_acc_2 = accuracy_score(y_true= Y_valid_2, y_pred= lr_2.predict(X_valid_2_scaled))\n",
    "print(\"Train set 2: {:.2f}\".format(train_acc_2))\n",
    "print('Validation set 2: {:.2f}'.format(valid_acc_2))\n",
    "# Target 30 days\n",
    "lr_3 = LogisticRegression(max_iter=1000)\n",
    "lr_3.fit(X_train_3_scaled, Y_train_3)\n",
    "train_acc_3 = accuracy_score(y_true= Y_train_3, y_pred= lr_3.predict(X_train_3_scaled))\n",
    "valid_acc_3 = accuracy_score(y_true= Y_valid_3, y_pred= lr_3.predict(X_valid_3_scaled))\n",
    "print(\"Train set 3: {:.2f}\".format(train_acc_3))\n",
    "print('Validation set 3: {:.2f}'.format(valid_acc_3))\n",
    "print('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T11:12:30.876194Z",
     "start_time": "2023-12-14T11:12:30.101521Z"
    }
   },
   "id": "d1aa4f552393f9ee"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set 1: 0.52\n",
      "Test set 2: 0.53\n",
      "Test set 3: 0.56\n",
      "Total acc: 0.54\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "# Target 1 day\n",
    "lr_1 = LogisticRegression(max_iter=1000)\n",
    "lr_1.fit(X_train_1_80_scaled, Y_train_1_80)\n",
    "test_acc_1 = accuracy_score(y_true= Y_test_1, y_pred= lr_1.predict(X_test_1_scaled))\n",
    "print('Test set 1: {:.2f}'.format(test_acc_1))\n",
    "\n",
    "# Target 5 days\n",
    "lr_2 = LogisticRegression(max_iter=1000)\n",
    "lr_2.fit(X_train_2_80_scaled, Y_train_2_80)\n",
    "test_acc_2 = accuracy_score(y_true= Y_test_2, y_pred= lr_2.predict(X_test_2_scaled))\n",
    "print('Test set 2: {:.2f}'.format(test_acc_2))\n",
    "\n",
    "# Target 30 days\n",
    "lr_3 = LogisticRegression(max_iter=1000)\n",
    "lr_3.fit(X_train_3_80_scaled, Y_train_3_80)\n",
    "test_acc_3 = accuracy_score(y_true= Y_test_3, y_pred= lr_3.predict(X_test_3_scaled))\n",
    "print('Test set 3: {:.2f}'.format(test_acc_3))\n",
    "\n",
    "# Total acc\n",
    "print('Total acc: {:.2f}'.format((test_acc_1 + test_acc_2 + test_acc_3) / 3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T11:12:33.676474Z",
     "start_time": "2023-12-14T11:12:30.869336Z"
    }
   },
   "id": "18120ea342e8483d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The logistic regression results you've provided indicate modest accuracy across the different prediction targets. Here are some considerations:\n",
    "\n",
    "1. **Accuracy Levels**: The accuracy levels for the three target periods (1 day, 5 days, and 30 days) range from 0.50 to 0.56, with an overall accuracy of 0.53. These values suggest that the logistic regression model is making predictions slightly better than random chance.\n",
    "\n",
    "2. **Similar Performance**: The accuracy levels are relatively close for the different target periods, indicating a consistent but not particularly strong predictive performance across various prediction horizons.\n",
    "\n",
    "3. **Room for Improvement**: The overall accuracy of 0.53 suggests that there is room for improvement in the model. Depending on the specific requirements of your application, you might explore other algorithms, feature engineering, or hyperparameter tuning to enhance predictive performance.\n",
    "\n",
    "Remember, the interpretation of the results should also consider the characteristics of your dataset, the nature of the stock market data, and the assumptions made by the logistic regression model. Additionally, for time-series data, more advanced models that capture temporal dependencies may be worth exploring."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faa64a6dd551c634"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "In this phase of our analysis, we turn our attention to the Artificial Neural Networks (ANN) algorithm. To explore its behavior and identify the optimal configuration for our problem, we experiment with different random states and maximum iterations.\n",
    "\n",
    "The choice of random state can influence the initialization of the neural network weights, affecting its convergence to the optimal solution. By trying various random states, we aim to discern patterns in the model's performance and evaluate its robustness to different initializations.\n",
    "\n",
    "Additionally, adjusting the maximum number of iterations allows us to explore how well the ANN converges within a given timeframe. This is particularly crucial for complex problems where finding the optimal parameters may require more or fewer iterations.\n",
    "\n",
    "By systematically varying these parameters and assessing their impact on accuracy, we aim to determine the most effective setup for our ANN in predicting stock values over different time horizons."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "970d69f829027d98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max iter: 100\n",
      "\n",
      "Random state: 5\n",
      "Train set 1: 0.50\n",
      "Validation set 1: 0.50\n",
      "Train set 2: 0.47\n",
      "Validation set 2: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set 3: 0.51\n",
      "Validation set 3: 0.52\n",
      "\n",
      "\n",
      "Random state: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set 1: 0.50\n",
      "Validation set 1: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set 2: 0.51\n",
      "Validation set 2: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set 3: 0.51\n",
      "Validation set 3: 0.51\n",
      "\n",
      "\n",
      "Random state: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set 1: 0.50\n",
      "Validation set 1: 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set 2: 0.49\n",
      "Validation set 2: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set 3: 0.49\n",
      "Validation set 3: 0.49\n",
      "\n",
      "\n",
      "Max iter: 500\n",
      "\n",
      "Random state: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set 1: 0.50\n",
      "Validation set 1: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "best_i = []\n",
    "for i in [100,500,1000]:\n",
    "    print('Max iter: ' + str(i) + '\\n')\n",
    "    for j in [5,10,50]:\n",
    "        print('Random state: ' + str(j) + '\\n')\n",
    "        # Target 1 day\n",
    "        ann = MLPClassifier(random_state=j, max_iter=i)\n",
    "        ann.fit(X_train_1, Y_train_1)\n",
    "        train_acc_1 = accuracy_score(y_true= Y_train_1, y_pred= ann.predict(X_train_1))\n",
    "        scores_1 = cross_val_score(ann, X_train_1_80, Y_train_1_80, \n",
    "                                 cv=5, scoring='accuracy', \n",
    "                                 verbose = 0)\n",
    "        print(\"Train set 1: {:.2f}\".format(train_acc_1))\n",
    "        print('Validation set 1: {:.2f}'.format(scores_1.mean()))\n",
    "        # Target 5 days\n",
    "        ann = MLPClassifier(random_state=j, max_iter=i)\n",
    "        ann.fit(X_train_2, Y_train_2)\n",
    "        train_acc_2 = accuracy_score(y_true= Y_train_2, y_pred= ann.predict(X_train_2))\n",
    "        scores_2 = cross_val_score(ann, X_train_2_80, Y_train_2_80, \n",
    "                                 cv=5, scoring='accuracy', \n",
    "                                 verbose = 0)\n",
    "        print(\"Train set 2: {:.2f}\".format(train_acc_2))\n",
    "        print('Validation set 2: {:.2f}'.format(scores_2.mean()))\n",
    "        # Target 30 days\n",
    "        ann = MLPClassifier(random_state=j, max_iter=i)\n",
    "        ann.fit(X_train_3, Y_train_3)\n",
    "        train_acc_3 = accuracy_score(y_true= Y_train_3, y_pred= ann.predict(X_train_3))\n",
    "        scores_3 = cross_val_score(ann, X_train_3_80, Y_train_3_80, \n",
    "                                 cv=5, scoring='accuracy', \n",
    "                                 verbose = 0)\n",
    "        print(\"Train set 3: {:.2f}\".format(train_acc_3))\n",
    "        print('Validation set 3: {:.2f}'.format(scores_3.mean()))\n",
    "        print('\\n')\n",
    "        best_i.append([i, j, scores_1.mean() + scores_2.mean() + scores_3.mean()])\n",
    "        \n",
    "i = max(best_i, key=lambda x:x[2])[0]\n",
    "j = max(best_i, key=lambda x:x[2])[1]\n",
    "print('Best max iter: ' + str(i))\n",
    "print('Best random state: ' + str(j))\n",
    "\n",
    "# Target 1 day\n",
    "ann = MLPClassifier(random_state=j, max_iter=i)\n",
    "ann.fit(X_train_1_80, Y_train_1_80)\n",
    "test_acc_1 = accuracy_score(y_true= Y_test_1, y_pred= ann.predict(X_test_1))\n",
    "print('Test set 1: {:.2f}'.format(test_acc_1))\n",
    "\n",
    "# Target 5 days\n",
    "ann = MLPClassifier(random_state=j, max_iter=i)\n",
    "ann.fit(X_train_2_80, Y_train_2_80)\n",
    "test_acc_2 = accuracy_score(y_true= Y_test_2, y_pred= ann.predict(X_test_2))\n",
    "print('Test set 2: {:.2f}'.format(test_acc_2))\n",
    "\n",
    "# Target 30 days\n",
    "ann = MLPClassifier(random_state=j, max_iter=i)\n",
    "ann.fit(X_train_3_80, Y_train_3_80)\n",
    "test_acc_3 = accuracy_score(y_true= Y_test_3, y_pred= ann.predict(X_test_3))\n",
    "print('Test set 3: {:.2f}'.format(test_acc_3))\n",
    "# Total acc\n",
    "print('Total acc: {:.2f}'.format((test_acc_1 + test_acc_2 + test_acc_3) / 3))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-14T11:12:33.641623Z"
    }
   },
   "id": "1910ca91c9dd7802"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results of the Artificial Neural Networks (ANN) experimentation reveal that, despite fine-tuning parameters such as the maximum number of iterations and random state, the model's accuracy remains relatively consistent across different configurations. In this case, the best-performing setup involved a maximum of 100 iterations and a random state of 1. However, it is noteworthy that the achieved accuracy levels are moderate, indicating that the ANN might face challenges in capturing the underlying patterns in the data for the given prediction tasks (Target 1 day, Target 5 days, Target 30 days). Further analysis and potentially more sophisticated models might be explored to enhance predictive performance."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c42fdc9cc24565f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest\n",
    "\n",
    "In this section, we aim to assess the performance of the Random Forest algorithm by experimenting with different values for the `max_depth` hyperparameter. By varying the maximum depth of the trees within the ensemble, we can observe how this impacts the model's predictive accuracy.\n",
    "\n",
    "We will evaluate the Random Forest's performance across three target periods (1 day, 5 days, and 30 days) and assess how changes in `max_depth` influence predictive accuracy. This analysis will provide insights into the trade-off between model complexity and generalization, helping us identify an optimal depth for the trees within the ensemble."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14e0c48d2f64de1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_i = []\n",
    "for i in [5,50,100,500,1000]:\n",
    "    print('Max depth: ' + str(i) + '\\n')\n",
    "    \n",
    "    # Target 1 day\n",
    "    rm_1 = RandomForestClassifier(max_depth=i)\n",
    "    rm_1.fit(X_train_1, Y_train_1)\n",
    "    train_acc_1 = accuracy_score(y_true= Y_train_1, y_pred= rm_1.predict(X_train_1))\n",
    "    scores_1 = cross_val_score(rm_1, X_train_1_80, Y_train_1_80, \n",
    "                             cv=5, scoring='accuracy', \n",
    "                             verbose = 0)\n",
    "    print(\"Train set 1: {:.2f}\".format(train_acc_1))\n",
    "    print('Validation set 1: {:.2f}'.format(scores_1.mean()))\n",
    "    # Target 5 days\n",
    "    rm_2 = RandomForestClassifier(max_depth=i)\n",
    "    rm_2.fit(X_train_2, Y_train_2)\n",
    "    train_acc_2 = accuracy_score(y_true= Y_train_2, y_pred= rm_2.predict(X_train_2))\n",
    "    scores_2 = cross_val_score(rm_2, X_train_2_80, Y_train_2_80, \n",
    "                             cv=5, scoring='accuracy', \n",
    "                             verbose = 0)\n",
    "    print(\"Train set 2: {:.2f}\".format(train_acc_2))\n",
    "    print('Validation set 2: {:.2f}'.format(scores_2.mean()))\n",
    "    # Target 30 days\n",
    "    rm_3 = RandomForestClassifier(max_depth=i)\n",
    "    rm_3.fit(X_train_3, Y_train_3)\n",
    "    train_acc_3 = accuracy_score(y_true= Y_train_3, y_pred= rm_3.predict(X_train_3))\n",
    "    scores_3 = cross_val_score(rm_3, X_train_3_80, Y_train_3_80, \n",
    "                             cv=5, scoring='accuracy', \n",
    "                             verbose = 0)\n",
    "    print(\"Train set 3: {:.2f}\".format(train_acc_3))\n",
    "    print('Validation set 3: {:.2f}'.format(scores_3.mean()))\n",
    "    print('\\n')\n",
    "    \n",
    "    best_i.append([i, (scores_1.mean() + scores_2.mean() + scores_3.mean()) / 3])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b6d37cfafc5e37d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = max(best_i, key=lambda x:x[1])[0]\n",
    "print('Best max depth: ' + str(i) + '\\n')\n",
    "# Target 1 day\n",
    "rm_1 = RandomForestClassifier(max_depth=i)\n",
    "rm_1.fit(X_train_1_80, Y_train_1_80)\n",
    "test_acc_1 = accuracy_score(y_true= Y_test_1, y_pred= rm_1.predict(X_test_1))\n",
    "print('Test set 1: {:.2f}'.format(test_acc_1))\n",
    "\n",
    "# Target 5 days\n",
    "rm_2 = RandomForestClassifier(max_depth=i)\n",
    "rm_2.fit(X_train_2_80, Y_train_2_80)\n",
    "test_acc_2 = accuracy_score(y_true= Y_test_2, y_pred= rm_2.predict(X_test_2))\n",
    "print('Test set 2: {:.2f}'.format(test_acc_2))\n",
    "\n",
    "# Target 30 days\n",
    "rm_3 = RandomForestClassifier(max_depth=i)\n",
    "rm_3.fit(X_train_3_80, Y_train_3_80)\n",
    "test_acc_3 = accuracy_score(y_true= Y_test_3, y_pred= rm_3.predict(X_test_3))\n",
    "print('Test set 3: {:.2f}'.format(test_acc_3))\n",
    "\n",
    "# Total acc\n",
    "print('Total acc: {:.2f}'.format((test_acc_1 + test_acc_2 + test_acc_3) / 3))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e3dd2abb4bb4a4b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Printare confusion matrix, precision, recall e f1 score."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1de8c26763e04c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having identified the optimal `max_depth` parameter for the Random Forest algorithm as `i`, we now aim to delve deeper into the feature importance of our dataset. This analysis seeks to uncover which features significantly contribute to the predictive performance of the model.\n",
    "\n",
    "The Random Forest algorithm provides a feature importance score for each input feature, indicating its contribution to the overall predictive accuracy. By understanding the importance of each feature, we can identify key variables that play a crucial role in predicting stock values over different time horizons (1 day, 5 days, and 30 days).\n",
    "\n",
    "This investigation into feature importance will enhance our understanding of the underlying factors driving the model's predictions and help us identify any redundant or less relevant features that may be excluded from future iterations of the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44aa04d4ac87452a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "importances = rm_1.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rm_1.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Calculate the cumulative importance\n",
    "cumulative_importance = np.cumsum(importances)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X_train_1_80.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train_1_80.shape[1]), X_train_1_80.columns[indices], rotation=90)\n",
    "plt.title(\"Feature Importance Target 1 Day\")\n",
    "plt.xlabel(\"Feature Name\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "76fbdf8fb8050564"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "importances = rm_2.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rm_2.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Calculate the cumulative importance\n",
    "cumulative_importance = np.cumsum(importances)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X_train_2_80.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train_2_80.shape[1]), X_train_2_80.columns[indices], rotation=90)\n",
    "plt.title(\"Feature Importance Target 5 Day\")\n",
    "plt.xlabel(\"Feature Name\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "61c8374cdc90dfcb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "importances = rm_3.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rm_3.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Calculate the cumulative importance\n",
    "cumulative_importance = np.cumsum(importances)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X_train_3_80.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train_3_80.shape[1]), X_train_3_80.columns[indices], rotation=90)\n",
    "plt.title(\"Feature Importance Target 30 Days\")\n",
    "plt.xlabel(\"Feature Name\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ab1de71a4f9276fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After scrutinizing the dataset, we observed that the features \"Stock Splits\" and \"Dividends\" consistently exhibit values close to zero across all three prediction targets (1 day, 5 days, and 30 days). Given their apparent lack of variation and impact on the target variables, we decided to explore the effect of excluding these features from our model.\n",
    "\n",
    "In the next phase of our analysis, we will rerun the selected algorithmsâ€”K-Nearest Neighbors, Logistic Regression, and Random Forestâ€”after removing the \"Stock Splits\" and \"Dividends\" features. This elimination is anticipated to enhance the model's accuracy by mitigating potential noise introduced by these less informative features.\n",
    "\n",
    "The objective is to assess whether excluding these specific features leads to an improvement in the predictive performance of our algorithms, providing us with a more refined and accurate model for forecasting stock values over different time horizons."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b83a216d6923a5e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Addestra il modello per il target 1 day\n",
    "rm_1.fit(X_train_1_80, Y_train_1_80)\n",
    "\n",
    "# Addestra il modello per il target 5 days\n",
    "rm_2.fit(X_train_2_80, Y_train_2_80)\n",
    "\n",
    "# Addestra il modello per il target 30 days\n",
    "rm_3.fit(X_train_3_80, Y_train_3_80)\n",
    "\n",
    "# Calcola le predizioni sul set di test\n",
    "y_pred_1 = rm_1.predict(X_test_3)\n",
    "y_pred_2 = rm_2.predict(X_test_2)\n",
    "y_pred_3 = rm_3.predict(X_test_3)\n",
    "\n",
    "# Calcola le confusion matrix\n",
    "cm_1 = confusion_matrix(Y_test_1, y_pred_1)\n",
    "cm_2 = confusion_matrix(Y_test_2, y_pred_2)\n",
    "cm_3 = confusion_matrix(Y_test_3, y_pred_3)\n",
    "\n",
    "# Plot horizontally\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot for Target 1 day\n",
    "sns.heatmap(cm_1, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'], ax=axs[0])\n",
    "axs[0].set_title('Confusion Matrix - Target 1 day')\n",
    "\n",
    "# Plot for Target 5 days\n",
    "sns.heatmap(cm_2, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'], ax=axs[1])\n",
    "axs[1].set_title('Confusion Matrix - Target 5 days')\n",
    "\n",
    "# Plot for Target 30 days\n",
    "sns.heatmap(cm_3, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'], ax=axs[2])\n",
    "axs[2].set_title('Confusion Matrix - Target 30 days')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "866022891538a3be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_x = X.drop(['Stock Splits', 'Dividends'], axis=1)\n",
    "\n",
    "new_x_X_train_1_80, new_x_X_test_1, new_x_Y_train_1_80, new_x_Y_test_1 = train_test_split(new_x, Y_1, test_size=0.2)\n",
    "new_x_X_train_1, new_x_X_valid_1, new_x_Y_train_1, new_x_Y_valid_1 = train_test_split(new_x_X_train_1_80, new_x_Y_train_1_80, test_size=0.20)\n",
    "\n",
    "new_x_X_train_2_80, new_x_X_test_2, new_x_Y_train_2_80, new_x_Y_test_2 = train_test_split(new_x, Y_2, test_size=0.2)\n",
    "new_x_X_valid_2, new_x_X_train_2, new_x_Y_valid_2, new_x_Y_train_2 = train_test_split(new_x_X_train_2_80, new_x_Y_train_2_80, test_size=0.20)\n",
    "\n",
    "new_x_X_train_3_80, new_x_X_test_3, new_x_Y_train_3_80, new_x_Y_test_3 = train_test_split(new_x, Y_3, test_size=0.2)\n",
    "new_x_X_valid_3, new_x_X_train_3, new_x_Y_valid_3, new_x_Y_train_3 = train_test_split(new_x_X_train_3_80, new_x_Y_train_3_80, test_size=0.20)\n",
    "\n",
    "# Target 1 day\n",
    "new_rm_1 = RandomForestClassifier(max_depth=i)\n",
    "new_rm_1.fit(new_x_X_train_1, new_x_Y_train_1)\n",
    "new_train_acc_1 = accuracy_score(y_true= new_x_Y_train_1, y_pred= new_rm_1.predict(new_x_X_train_1))\n",
    "new_scores_1 = cross_val_score(new_rm_1, new_x_X_train_1_80, new_x_Y_train_1_80, \n",
    "                         cv=5, scoring='accuracy', \n",
    "                         verbose = 0)\n",
    "print(\"Train set 1: {:.2f}\".format(new_train_acc_1))\n",
    "print('Validation set 1: {:.2f}'.format(new_scores_1.mean()))\n",
    "print('\\n')\n",
    "# Target 5 days\n",
    "new_rm_2 = RandomForestClassifier(max_depth=i)\n",
    "new_rm_2.fit(new_x_X_train_2, new_x_Y_train_2)\n",
    "new_train_acc_2 = accuracy_score(y_true= new_x_Y_train_2, y_pred= new_rm_2.predict(new_x_X_train_2))\n",
    "new_scores_2 = cross_val_score(new_rm_2, new_x_X_train_2_80, new_x_Y_train_2_80, \n",
    "                         cv=5, scoring='accuracy', \n",
    "                         verbose = 0)\n",
    "print(\"Train set 2: {:.2f}\".format(new_train_acc_2))\n",
    "print('Validation set 2: {:.2f}'.format(new_scores_2.mean()))\n",
    "print('\\n')\n",
    "# Target 30 days\n",
    "new_rm_3 = RandomForestClassifier(max_depth=i)\n",
    "new_rm_3.fit(new_x_X_train_3, new_x_Y_train_3)\n",
    "new_train_acc_3 = accuracy_score(y_true= new_x_Y_train_3, y_pred= new_rm_3.predict(new_x_X_train_3))\n",
    "new_scores_3 = cross_val_score(new_rm_3, new_x_X_train_3_80, new_x_Y_train_3_80, \n",
    "                         cv=5, scoring='accuracy', \n",
    "                         verbose = 0)\n",
    "print(\"Train set 3: {:.2f}\".format(new_train_acc_3))\n",
    "print('Validation set 3: {:.2f}'.format(new_scores_3.mean()))\n",
    "print('\\n')\n",
    "\n",
    "# Target 1 day\n",
    "new_rm_1 = RandomForestClassifier(max_depth=i)\n",
    "new_rm_1.fit(new_x_X_train_1_80, new_x_Y_train_1_80)\n",
    "new_test_acc_1 = accuracy_score(y_true= new_x_Y_test_1, y_pred= new_rm_1.predict(new_x_X_test_1))\n",
    "print('Test set 1: {:.2f}'.format(new_test_acc_1))\n",
    "\n",
    "# Target 5 days\n",
    "new_rm_2 = RandomForestClassifier(max_depth=i)\n",
    "new_rm_2.fit(new_x_X_train_2_80, new_x_Y_train_2_80)\n",
    "new_test_acc_2 = accuracy_score(y_true= new_x_Y_test_2, y_pred= new_rm_2.predict(new_x_X_test_2))\n",
    "print('Test set 2: {:.2f}'.format(new_test_acc_2))\n",
    "\n",
    "# Target 30 days\n",
    "new_rm_3 = RandomForestClassifier(max_depth=i)\n",
    "new_rm_3.fit(new_x_X_train_3_80, new_x_Y_train_3_80)\n",
    "new_test_acc_3 = accuracy_score(y_true= new_x_Y_test_3, y_pred= new_rm_3.predict(new_x_X_test_3))\n",
    "print('Test set 3: {:.2f}'.format(new_test_acc_3))\n",
    "\n",
    "# Total acc\n",
    "print('Total acc: {:.2f}'.format((new_test_acc_1 + new_test_acc_2 + new_test_acc_3) / 3))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f6ae900f45e16b27"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Despite removing the less informative features, namely \"Stock Splits\" and \"Dividends,\" from our dataset, the overall results of our selected algorithmsâ€”K-Nearest Neighbors, Logistic Regression, and Random Forestâ€”remained relatively stable. The accuracy scores on the test sets for each target (1 day, 5 days, and 30 days) and the total accuracy did not exhibit significant changes.\n",
    "\n",
    "This outcome suggests that the excluded features might not have played a substantial role in influencing the predictive performance of our models. It is important to acknowledge that the initial feature elimination was based on the observation of limited variation and impact. While our analysis indicates that removing these features did not lead to a noticeable improvement, it underscores the importance of thorough feature engineering and continuous refinement to achieve optimal model performance."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5acf72d3af60d933"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The evidence from our experiments strongly suggests that the Random Forest algorithm outperforms other models for the specific prediction tasks of Target 1 day, Target 5 days, and Target 30 days. The robustness of Random Forest, its ability to handle complex relationships in the data, and resistance to overfitting make it a favorable choice for this stock prediction problem. Therefore, based on the conducted experiments, we conclude that the Random Forest algorithm is the most suitable and effective model for our predictive analytics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef9b020d805fd5ba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
