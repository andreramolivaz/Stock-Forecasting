{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train\n",
    "\n",
    "Algoritmi:\n",
    "\n",
    "**K-Nearest Neighbors (KNN)**:\n",
    "Pro: Semplice da implementare, non richiede addestramento costoso.\n",
    "Contro: Prestazioni possono decadere con dataset di grandi dimensioni. Sensibile alla scala delle feature.\n",
    "\n",
    "**Decision Trees**:\n",
    "Pro: Facile da interpretare, gestisce automaticamente le feature rilevanti.\n",
    "Contro: Tendenza all'overfitting. Può essere instabile con piccole variazioni nei dati.\n",
    "\n",
    "**Linear Regression**:\n",
    "Pro: Semplice e interpretabile, adatta per relazioni lineari tra feature e target.\n",
    "Contro: Sensibile a outliers. Non gestisce bene relazioni complesse.\n",
    "\n",
    "**Logistic Regression**:\n",
    "Pro: Buona per problemi di classificazione binaria.\n",
    "Contro: Assume una relazione lineare tra le feature e il log-odds. Non gestisce bene relazioni complesse.\n",
    "\n",
    "**Support Vector Machines (SVM)**:\n",
    "Pro: Buone prestazioni in spazi delle feature ad alta dimensione.\n",
    "Contro: Richiede una scelta accurata dei parametri. Non sempre efficace con dataset molto grandi.\n",
    "\n",
    "**Random Forest**:\n",
    "Pro: Buona capacità di gestire complessità e overfitting. Può fornire importanza delle feature.\n",
    "Contro: Meno interpretabile rispetto ai singoli alberi. Può richiedere tempo per l'addestramento.\n",
    "\n",
    "**Ensemble Methods**:\n",
    "Pro: Combina diversi modelli per migliorare le prestazioni complessive.\n",
    "Contro: Complessità e interpretabilità possono essere un problema.\n",
    "\n",
    "**Neural Networks**:\n",
    "Pro: Eccellenti per problemi complessi e non lineari. Addestramento su grandi quantità di dati.\n",
    "Contro: Richiede molto dati e risorse di calcolo. Complessità nella scelta dell'architettura.\n",
    "\n",
    "**Convolutional Neural Networks (CNN)**:\n",
    "Pro: Eccellenti per dati strutturati come immagini. Applicabili anche a dati sequenziali.\n",
    "Contro: Richiede dati etichettati in grandi quantità.\n",
    "\n",
    "Escludiamo quindi Linear Regression, Decision Trees,Support Vector Machines (SVM) e Convolutional Neural Networks (CNN).\n",
    "\n",
    "Considerazioni:\n",
    "\n",
    "Possiamo utilizzare i rimanenti algoritmi per dividere il problema il 4 categorie: Unsafe, Target 1 day, Target 5 days, Target 30 days.\n",
    "\n",
    "**Knn**:\n",
    "La scelta del K è importante e può variare molto prendendo due K molto vicini.\n",
    "Per questo motivo abbiamo deciso di affidarci ad altri tipi di algoritmi per questo tipo di problema.\n",
    " \n",
    "**Logistic Regression**: \n",
    "Possiamo utilizzare la Regressione Logistica per dividere il problema in 3 sottoproblemi. Possiamo classificare se le predizioni appartengono o meno all'etichetta Target 1 day, Target 5 days o Target 30 days. Se tutte e 3 dovessero essere 0, la scelta ricadrebbe su Unsafe.\n",
    "Questo vorrebbe dire allenare 3 volte l'algoritmo predicendo una alla volta tutte e tre le etichette.\n",
    "(Da testare)\n",
    "\n",
    "**Random Forests**:\n",
    "Possiamo utilizzare la tecnica del Bagging per ridurre la varianza, tipicamente elevata con questo algoritmo, per avere un accuracy più alta.\n",
    "(Da testare)\n",
    "\n",
    "**Ensemble Methods**: \n",
    "(Che metodi utilizzare?)\n",
    "(Da Testare)\n",
    "\n",
    "**Neural Networks**: \n",
    "L'utilizzo delle neural networks risulta essere più complicato degli altri poiché è più difficile da costruire e può svilupparsi in diverse varianti.\n",
    "La parte difficile appunto sta nella scelta del numero di hidden layers e nel numero di nodi di questi. Tale scelta può essere affinata con test ripetuti per arrivare ad una soluzione comune.\n",
    "(Da testare)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28918e1ed52316e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc15b3fd4a850708"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_blobs\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T16:56:24.883569Z",
     "start_time": "2023-11-23T16:56:24.591805Z"
    }
   },
   "id": "45384eb0b2c9c55a"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "            Date          Open          High           Low         Close  \\\n0     2020-06-30  90534.565179  90717.278731  89255.570312  89255.570312   \n1     2020-07-01  89986.421883  90899.989618  89529.638016  89712.351562   \n2     2020-07-02  89529.635417  91082.700521  89529.635417  90443.203125   \n3     2020-07-03  91265.419308  91813.559964  90077.781218  90625.921875   \n4     2020-07-06  91082.695405  93640.684845  90260.484514  92727.117188   \n...          ...           ...           ...           ...           ...   \n26675 2023-10-20    112.919998    113.320000    110.790001    111.080002   \n26676 2023-10-23    110.629997    110.959999    108.680000    109.449997   \n26677 2023-10-24    109.699997    109.820000    108.120003    108.389999   \n26678 2023-10-25    108.519997    109.500000    108.129997    108.589996   \n26679 2023-10-26    107.449997    108.339996    106.495003    107.425003   \n\n         Volume  Dividends  Stock Splits  Daily_Return  Target_1day  ...  \\\n0        988987        0.0           0.0      0.000000            1  ...   \n1        640540        0.0           0.0      0.005118            1  ...   \n2        730963        0.0           0.0      0.008147            1  ...   \n3        569575        0.0           0.0      0.002020            1  ...   \n4       1189877        0.0           0.0      0.023185            0  ...   \n...         ...        ...           ...           ...          ...  ...   \n26675  22439800        0.0           0.0     -0.017165            0  ...   \n26676  18185000        0.0           0.0     -0.014674            0  ...   \n26677  16786100        0.0           0.0     -0.009685            1  ...   \n26678  22047300        0.0           0.0      0.001845            0  ...   \n26679  12054832        0.0           0.0     -0.010728            0  ...   \n\n              MA_30         MA_50        RSI        MACD  Signal_Line  \\\n0      92851.977604  89900.544844  29.090917 -314.945626   517.337196   \n1      92879.384896  89858.520781  33.114757 -434.587634   326.952230   \n2      92934.198698  89884.100625  41.444863 -465.070135   168.547757   \n3      93077.324479  89988.247500  55.500008 -469.076637    41.022878   \n4      93244.811979  90150.862500  50.000000 -299.253312   -27.032360   \n...             ...           ...        ...         ...          ...   \n26675    113.866000    112.431415  41.095898   -0.672795    -0.697816   \n26676    113.709000    112.402000  38.176416   -0.775562    -0.713365   \n26677    113.405666    112.349600  43.441582   -0.931798    -0.757052   \n26678    113.143999    112.358200  49.065416   -1.027632    -0.811168   \n26679    112.774166    112.379900  50.614618   -1.183938    -0.885722   \n\n       Bollinger_Mid_Band  Bollinger_Upper_Band  Bollinger_Lower_Band  \\\n0            94577.098437         104488.275493          84665.921382   \n1            94106.611328         104003.310201          84209.912455   \n2            93672.666797         103409.739151          83935.594443   \n3            93133.661719         102267.155198          84000.168239   \n4            92608.360156         100464.250846          84752.469466   \n...                   ...                   ...                   ...   \n26675          112.580000            120.644831            104.515168   \n26676          112.240999            120.232491            104.249507   \n26677          111.839999            119.758208            103.921790   \n26678          111.259499            118.283746            104.235252   \n26679          110.657249            116.757753            104.556745   \n\n       Volatility    Ticker  \n0        0.030295  005380KS  \n1        0.018542  005380KS  \n2        0.012799  005380KS  \n3        0.012387  005380KS  \n4        0.009195  005380KS  \n...           ...       ...  \n26675    0.012627       XOM  \n26676    0.014787       XOM  \n26677    0.012802       XOM  \n26678    0.008695       XOM  \n26679    0.007316       XOM  \n\n[26680 rows x 36 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>Dividends</th>\n      <th>Stock Splits</th>\n      <th>Daily_Return</th>\n      <th>Target_1day</th>\n      <th>...</th>\n      <th>MA_30</th>\n      <th>MA_50</th>\n      <th>RSI</th>\n      <th>MACD</th>\n      <th>Signal_Line</th>\n      <th>Bollinger_Mid_Band</th>\n      <th>Bollinger_Upper_Band</th>\n      <th>Bollinger_Lower_Band</th>\n      <th>Volatility</th>\n      <th>Ticker</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-06-30</td>\n      <td>90534.565179</td>\n      <td>90717.278731</td>\n      <td>89255.570312</td>\n      <td>89255.570312</td>\n      <td>988987</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>...</td>\n      <td>92851.977604</td>\n      <td>89900.544844</td>\n      <td>29.090917</td>\n      <td>-314.945626</td>\n      <td>517.337196</td>\n      <td>94577.098437</td>\n      <td>104488.275493</td>\n      <td>84665.921382</td>\n      <td>0.030295</td>\n      <td>005380KS</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-07-01</td>\n      <td>89986.421883</td>\n      <td>90899.989618</td>\n      <td>89529.638016</td>\n      <td>89712.351562</td>\n      <td>640540</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.005118</td>\n      <td>1</td>\n      <td>...</td>\n      <td>92879.384896</td>\n      <td>89858.520781</td>\n      <td>33.114757</td>\n      <td>-434.587634</td>\n      <td>326.952230</td>\n      <td>94106.611328</td>\n      <td>104003.310201</td>\n      <td>84209.912455</td>\n      <td>0.018542</td>\n      <td>005380KS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-07-02</td>\n      <td>89529.635417</td>\n      <td>91082.700521</td>\n      <td>89529.635417</td>\n      <td>90443.203125</td>\n      <td>730963</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.008147</td>\n      <td>1</td>\n      <td>...</td>\n      <td>92934.198698</td>\n      <td>89884.100625</td>\n      <td>41.444863</td>\n      <td>-465.070135</td>\n      <td>168.547757</td>\n      <td>93672.666797</td>\n      <td>103409.739151</td>\n      <td>83935.594443</td>\n      <td>0.012799</td>\n      <td>005380KS</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-07-03</td>\n      <td>91265.419308</td>\n      <td>91813.559964</td>\n      <td>90077.781218</td>\n      <td>90625.921875</td>\n      <td>569575</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.002020</td>\n      <td>1</td>\n      <td>...</td>\n      <td>93077.324479</td>\n      <td>89988.247500</td>\n      <td>55.500008</td>\n      <td>-469.076637</td>\n      <td>41.022878</td>\n      <td>93133.661719</td>\n      <td>102267.155198</td>\n      <td>84000.168239</td>\n      <td>0.012387</td>\n      <td>005380KS</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-07-06</td>\n      <td>91082.695405</td>\n      <td>93640.684845</td>\n      <td>90260.484514</td>\n      <td>92727.117188</td>\n      <td>1189877</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.023185</td>\n      <td>0</td>\n      <td>...</td>\n      <td>93244.811979</td>\n      <td>90150.862500</td>\n      <td>50.000000</td>\n      <td>-299.253312</td>\n      <td>-27.032360</td>\n      <td>92608.360156</td>\n      <td>100464.250846</td>\n      <td>84752.469466</td>\n      <td>0.009195</td>\n      <td>005380KS</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>26675</th>\n      <td>2023-10-20</td>\n      <td>112.919998</td>\n      <td>113.320000</td>\n      <td>110.790001</td>\n      <td>111.080002</td>\n      <td>22439800</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.017165</td>\n      <td>0</td>\n      <td>...</td>\n      <td>113.866000</td>\n      <td>112.431415</td>\n      <td>41.095898</td>\n      <td>-0.672795</td>\n      <td>-0.697816</td>\n      <td>112.580000</td>\n      <td>120.644831</td>\n      <td>104.515168</td>\n      <td>0.012627</td>\n      <td>XOM</td>\n    </tr>\n    <tr>\n      <th>26676</th>\n      <td>2023-10-23</td>\n      <td>110.629997</td>\n      <td>110.959999</td>\n      <td>108.680000</td>\n      <td>109.449997</td>\n      <td>18185000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.014674</td>\n      <td>0</td>\n      <td>...</td>\n      <td>113.709000</td>\n      <td>112.402000</td>\n      <td>38.176416</td>\n      <td>-0.775562</td>\n      <td>-0.713365</td>\n      <td>112.240999</td>\n      <td>120.232491</td>\n      <td>104.249507</td>\n      <td>0.014787</td>\n      <td>XOM</td>\n    </tr>\n    <tr>\n      <th>26677</th>\n      <td>2023-10-24</td>\n      <td>109.699997</td>\n      <td>109.820000</td>\n      <td>108.120003</td>\n      <td>108.389999</td>\n      <td>16786100</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.009685</td>\n      <td>1</td>\n      <td>...</td>\n      <td>113.405666</td>\n      <td>112.349600</td>\n      <td>43.441582</td>\n      <td>-0.931798</td>\n      <td>-0.757052</td>\n      <td>111.839999</td>\n      <td>119.758208</td>\n      <td>103.921790</td>\n      <td>0.012802</td>\n      <td>XOM</td>\n    </tr>\n    <tr>\n      <th>26678</th>\n      <td>2023-10-25</td>\n      <td>108.519997</td>\n      <td>109.500000</td>\n      <td>108.129997</td>\n      <td>108.589996</td>\n      <td>22047300</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.001845</td>\n      <td>0</td>\n      <td>...</td>\n      <td>113.143999</td>\n      <td>112.358200</td>\n      <td>49.065416</td>\n      <td>-1.027632</td>\n      <td>-0.811168</td>\n      <td>111.259499</td>\n      <td>118.283746</td>\n      <td>104.235252</td>\n      <td>0.008695</td>\n      <td>XOM</td>\n    </tr>\n    <tr>\n      <th>26679</th>\n      <td>2023-10-26</td>\n      <td>107.449997</td>\n      <td>108.339996</td>\n      <td>106.495003</td>\n      <td>107.425003</td>\n      <td>12054832</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.010728</td>\n      <td>0</td>\n      <td>...</td>\n      <td>112.774166</td>\n      <td>112.379900</td>\n      <td>50.614618</td>\n      <td>-1.183938</td>\n      <td>-0.885722</td>\n      <td>110.657249</td>\n      <td>116.757753</td>\n      <td>104.556745</td>\n      <td>0.007316</td>\n      <td>XOM</td>\n    </tr>\n  </tbody>\n</table>\n<p>26680 rows × 36 columns</p>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.ExcelFile('final_dataset.xlsx').parse('Sheet1')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T16:56:41.915003Z",
     "start_time": "2023-11-23T16:56:24.599372Z"
    }
   },
   "id": "956e36475c7e88af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Knn\n",
    "\n",
    "Dobbiamo convertire tutte le feature in float per poter utilizzare l'algoritmo."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbe8fe2d4d40732f"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# Converti date in float\n",
    "df['Date'] = pd.to_datetime(df['Date']).astype(int) / 10**9\n",
    "df['Ticker'] = pd.factorize(df.Ticker)[0]\n",
    "df['Volume'] = df['Volume'].astype(float)\n",
    "df['Target_1day'] = df['Target_1day'].astype(float)\n",
    "df['Target_5days'] = df['Target_5days'].astype(float)\n",
    "df['Target_30days'] = df['Target_30days'].astype(float)\n",
    "df['Net Income'] = df['Net Income'].astype(float)\n",
    "df['Total Revenue'] = df['Total Revenue'].astype(float)\n",
    "df['Normalized EBITDA'] = df['Normalized EBITDA'].astype(float)\n",
    "df['Total Unusual Items'] = df['Total Unusual Items'].astype(float)\n",
    "df['Total Unusual Items Excluding Goodwill'] = df['Total Unusual Items Excluding Goodwill'].astype(float)\n",
    "df['Operating Cash Flow'] = df['Operating Cash Flow'].astype(float)\n",
    "df['Capital Expenditure'] = df['Capital Expenditure'].astype(float)\n",
    "df['Free Cash Flow'] = df['Free Cash Flow'].astype(float)\n",
    "df['Cash Flow From Continuing Operating Activities'] = df['Cash Flow From Continuing Operating Activities'].astype(float)\n",
    "df['Cash Flow From Continuing Investing Activities'] = df['Cash Flow From Continuing Investing Activities'].astype(float)\n",
    "df['Cash Flow From Continuing Financing Activities'] = df['Cash Flow From Continuing Financing Activities'].astype(float)\n",
    "df['Ticker'] = df['Ticker'].astype(float)\n",
    "\n",
    "# divide test and train\n",
    "X = df.drop(['Target_1day', 'Target_5days', 'Target_30days'], axis=1)\n",
    "Y = df[['Target_1day', 'Target_5days', 'Target_30days']]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T16:56:42.080142Z",
     "start_time": "2023-11-23T16:56:41.906813Z"
    }
   },
   "id": "62674f7923e6c357"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for KNN with K = 1 is 0.3779985007496252\n",
      "Accuracy for KNN with K = 3 is 0.35344827586206895\n",
      "Accuracy for KNN with K = 5 is 0.30603448275862066\n",
      "Accuracy for KNN with K = 7 is 0.3154047976011994\n",
      "Accuracy for KNN with K = 9 is 0.30359820089955025\n",
      "Accuracy for KNN with K = 11 is 0.29685157421289354\n",
      "Accuracy for KNN with K = 13 is 0.2946026986506747\n",
      "Accuracy for KNN with K = 15 is 0.2912293853073463\n",
      "Accuracy for KNN with K = 17 is 0.2901049475262369\n",
      "Accuracy for KNN with K = 19 is 0.28523238380809596\n",
      "Accuracy for KNN with K = 21 is 0.2848575712143928\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i in [1,3,5,7,9,11,13,15,17,19,21]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    Y_pred = knn.predict(X_test)\n",
    "    print(\"Accuracy for KNN with K = \" + str(i) + \" is \" + str(accuracy_score(Y_test, Y_pred)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T16:56:58.858490Z",
     "start_time": "2023-11-23T16:56:42.008857Z"
    }
   },
   "id": "2e260f77946702c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Dividiamo il nostro problema di classificazione in 3 sottoproblemi:\n",
    "\n",
    "- X_1 e Y_1: Target_1day\n",
    "- X_2 e Y_2: Target_5days\n",
    "- X_3 e Y_3: Target_30days\n",
    "\n",
    "In questo modo per ogni sottoproblema possiamo allenare un modello di regressione logistica per classificare se una predizione appartiene o meno all'etichetta."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7c7e8e97b2f33dd"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_1 = df.drop('Target_1day', axis=1)\n",
    "Y_1 = df['Target_1day']\n",
    "\n",
    "X_train_1_80, X_test_1, Y_train_1_80, Y_test_1 = train_test_split(X_1, Y_1, test_size=0.2)\n",
    "X_train_1, X_valid_1, Y_train_1, Y_valid_1 = train_test_split(X_train_1_80, Y_train_1_80, test_size=0.20)\n",
    "\n",
    "X_2 = df.drop('Target_5days', axis=1)\n",
    "Y_2 = df['Target_5days']\n",
    "\n",
    "X_train_2_80, X_test_2, Y_train_2_80, Y_test_2 = train_test_split(X_2, Y_2, test_size=0.2)\n",
    "X_valid_2, X_train_2, Y_valid_2, Y_train_2 = train_test_split(X_train_2_80, Y_train_2_80, test_size=0.20)\n",
    "\n",
    "X_3 = df.drop('Target_30days', axis=1)\n",
    "Y_3 = df['Target_30days']\n",
    "\n",
    "X_train_3_80, X_test_3, Y_train_3_80, Y_test_3 = train_test_split(X_3, Y_3, test_size=0.2)\n",
    "X_valid_3, X_train_3, Y_valid_3, Y_train_3 = train_test_split(X_train_3_80, Y_train_3_80, test_size=0.20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T16:56:59.010127Z",
     "start_time": "2023-11-23T16:56:58.869140Z"
    }
   },
   "id": "4b462dcee02cc58b"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set 1: 0.51\n",
      "Validation set 1: 0.51\n",
      "Test set 1: 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_1, Y_train_1)\n",
    "train_acc = accuracy_score(y_true= Y_train_1, y_pred= logreg.predict(X_train_1))\n",
    "scores = cross_val_score(logreg, X_train_1_80, Y_train_1_80, \n",
    "                         cv=5, scoring='accuracy', \n",
    "                         verbose = 0)\n",
    "print(\"Train set 1: {:.2f}\".format(train_acc))\n",
    "print('Validation set 1: {:.2f}'.format(scores.mean()))\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_1_80, Y_train_1_80)\n",
    "test_acc = accuracy_score(y_true= Y_test_1, y_pred= logreg.predict(X_test_1))\n",
    "\n",
    "print('Test set 1: {:.2f}'.format(test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T16:57:01.156993Z",
     "start_time": "2023-11-23T16:56:58.999703Z"
    }
   },
   "id": "4b28c56c224183ad"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set 2: 0.52\n",
      "Validation set 2: 0.51\n",
      "Test set 2: 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_2, Y_train_2)\n",
    "train_acc = accuracy_score(y_true= Y_train_2, y_pred= logreg.predict(X_train_2))\n",
    "scores = cross_val_score(logreg, X_train_2_80, Y_train_2_80, \n",
    "                         cv=5, scoring='accuracy', \n",
    "                         verbose = 0)\n",
    "print(\"Train set 2: {:.2f}\".format(train_acc))\n",
    "print('Validation set 2: {:.2f}'.format(scores.mean()))\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_2_80, Y_train_2_80)\n",
    "test_acc = accuracy_score(y_true= Y_test_2, y_pred= logreg.predict(X_test_2))\n",
    "\n",
    "print('Test set 2: {:.2f}'.format(test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T16:57:03.156555Z",
     "start_time": "2023-11-23T16:57:01.131042Z"
    }
   },
   "id": "1b8c2c1491f788d7"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set 3: 0.55\n",
      "Validation set 3: 0.52\n",
      "Test set 3: 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_3, Y_train_3)\n",
    "train_acc = accuracy_score(y_true= Y_train_3, y_pred= logreg.predict(X_train_3))\n",
    "scores = cross_val_score(logreg, X_train_3_80, Y_train_3_80, \n",
    "                         cv=5, scoring='accuracy', \n",
    "                         verbose = 0)\n",
    "print(\"Train set 3: {:.2f}\".format(train_acc))\n",
    "print('Validation set 3: {:.2f}'.format(scores.mean()))\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_3_80, Y_train_3_80)\n",
    "test_acc = accuracy_score(y_true= Y_test_3, y_pred= logreg.predict(X_test_3))\n",
    "\n",
    "print('Test set 3: {:.2f}'.format(test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T16:57:05.992508Z",
     "start_time": "2023-11-23T16:57:03.155246Z"
    }
   },
   "id": "3348c63a4ce6a822"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14e0c48d2f64de1"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[81], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m rm_1\u001B[38;5;241m.\u001B[39mfit(X_train_1, Y_train_1)\n\u001B[1;32m     10\u001B[0m train_acc_1 \u001B[38;5;241m=\u001B[39m accuracy_score(y_true\u001B[38;5;241m=\u001B[39m Y_train_1, y_pred\u001B[38;5;241m=\u001B[39m rm_1\u001B[38;5;241m.\u001B[39mpredict(X_train_1))\n\u001B[0;32m---> 11\u001B[0m scores_1 \u001B[38;5;241m=\u001B[39m \u001B[43mcross_val_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrm_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train_1_80\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train_1_80\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43maccuracy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain set 1: \u001B[39m\u001B[38;5;132;01m{:.2f}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(train_acc_1))\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mValidation set 1: \u001B[39m\u001B[38;5;132;01m{:.2f}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(scores_1\u001B[38;5;241m.\u001B[39mmean()))\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:562\u001B[0m, in \u001B[0;36mcross_val_score\u001B[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001B[0m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;66;03m# To ensure multimetric format is not supported\u001B[39;00m\n\u001B[1;32m    560\u001B[0m scorer \u001B[38;5;241m=\u001B[39m check_scoring(estimator, scoring\u001B[38;5;241m=\u001B[39mscoring)\n\u001B[0;32m--> 562\u001B[0m cv_results \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    566\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mscore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    568\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    569\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    570\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    571\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    572\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpre_dispatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_dispatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    573\u001B[0m \u001B[43m    \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    574\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    575\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cv_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_score\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    207\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    208\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    209\u001B[0m         )\n\u001B[1;32m    210\u001B[0m     ):\n\u001B[0;32m--> 211\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    213\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    218\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    219\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    221\u001B[0m     )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:309\u001B[0m, in \u001B[0;36mcross_validate\u001B[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001B[0m\n\u001B[1;32m    306\u001B[0m \u001B[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001B[39;00m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;66;03m# independent, and that it is pickle-able.\u001B[39;00m\n\u001B[1;32m    308\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[0;32m--> 309\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    310\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscorers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    320\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    321\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_times\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    322\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_estimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_estimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    323\u001B[0m \u001B[43m        \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    324\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\n\u001B[1;32m    326\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    328\u001B[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001B[1;32m    330\u001B[0m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     60\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     61\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     62\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     64\u001B[0m )\n\u001B[0;32m---> 65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/joblib/parallel.py:1863\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1861\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[1;32m   1862\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[1;32m   1865\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[1;32m   1866\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[1;32m   1867\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[1;32m   1868\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[1;32m   1869\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[1;32m   1870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/joblib/parallel.py:1792\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1790\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1791\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1792\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1793\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1794\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    125\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:729\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[1;32m    727\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[1;32m    728\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 729\u001B[0m         \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    731\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    732\u001B[0m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n\u001B[1;32m    733\u001B[0m     fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1150\u001B[0m     )\n\u001B[1;32m   1151\u001B[0m ):\n\u001B[0;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:456\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    445\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    446\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[1;32m    447\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[1;32m    448\u001B[0m ]\n\u001B[1;32m    450\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[1;32m    451\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[1;32m    454\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[0;32m--> 456\u001B[0m trees \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    458\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    459\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthreads\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    461\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_parallel_build_trees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    462\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    463\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    464\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    465\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    466\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[43m        \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    468\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    469\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    470\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    471\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    472\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    473\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    474\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    476\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     60\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     61\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     62\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     64\u001B[0m )\n\u001B[0;32m---> 65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/joblib/parallel.py:1863\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1861\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[1;32m   1862\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[1;32m   1865\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[1;32m   1866\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[1;32m   1867\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[1;32m   1868\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[1;32m   1869\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[1;32m   1870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/joblib/parallel.py:1792\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1790\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1791\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1792\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1793\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1794\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    125\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:188\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001B[0m\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m class_weight \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced_subsample\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    186\u001B[0m         curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m, y, indices\u001B[38;5;241m=\u001B[39mindices)\n\u001B[0;32m--> 188\u001B[0m     \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurr_sample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    190\u001B[0m     tree\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39msample_weight, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1150\u001B[0m     )\n\u001B[1;32m   1151\u001B[0m ):\n\u001B[0;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/tree/_classes.py:959\u001B[0m, in \u001B[0;36mDecisionTreeClassifier.fit\u001B[0;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[1;32m    928\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    929\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m    930\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001B[39;00m\n\u001B[1;32m    931\u001B[0m \n\u001B[1;32m    932\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    956\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[1;32m    957\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 959\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    960\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    961\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    962\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    963\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    964\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    965\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/sklearn/tree/_classes.py:295\u001B[0m, in \u001B[0;36mBaseDecisionTree._fit\u001B[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001B[0m\n\u001B[1;32m    293\u001B[0m y_encoded \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(y\u001B[38;5;241m.\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mint\u001B[39m)\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_):\n\u001B[0;32m--> 295\u001B[0m     classes_k, y_encoded[:, k] \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inverse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_\u001B[38;5;241m.\u001B[39mappend(classes_k)\n\u001B[1;32m    297\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_\u001B[38;5;241m.\u001B[39mappend(classes_k\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/numpy/lib/arraysetops.py:274\u001B[0m, in \u001B[0;36munique\u001B[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001B[0m\n\u001B[1;32m    272\u001B[0m ar \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masanyarray(ar)\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 274\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43m_unique1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inverse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mequal_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mequal_nan\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _unpack_tuple(ret)\n\u001B[1;32m    278\u001B[0m \u001B[38;5;66;03m# axis was specified and not None\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/General/lib/python3.11/site-packages/numpy/lib/arraysetops.py:338\u001B[0m, in \u001B[0;36m_unique1d\u001B[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001B[0m\n\u001B[1;32m    336\u001B[0m     ar\u001B[38;5;241m.\u001B[39msort()\n\u001B[1;32m    337\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar\n\u001B[0;32m--> 338\u001B[0m mask \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty(aux\u001B[38;5;241m.\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mbool_)\n\u001B[1;32m    339\u001B[0m mask[:\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    340\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (equal_nan \u001B[38;5;129;01mand\u001B[39;00m aux\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m aux\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcfmM\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    341\u001B[0m         np\u001B[38;5;241m.\u001B[39misnan(aux[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "best_i = []\n",
    "for i in [2,10,30,50,70,100,500,1000]:\n",
    "    print('Max depth: ' + str(i) + '\\n')\n",
    "    \n",
    "    # Target 1 day\n",
    "    rm_1 = RandomForestClassifier(max_depth=i)\n",
    "    rm_1.fit(X_train_1, Y_train_1)\n",
    "    train_acc_1 = accuracy_score(y_true= Y_train_1, y_pred= rm_1.predict(X_train_1))\n",
    "    scores_1 = cross_val_score(rm_1, X_train_1_80, Y_train_1_80, \n",
    "                             cv=5, scoring='accuracy', \n",
    "                             verbose = 0)\n",
    "    print(\"Train set 1: {:.2f}\".format(train_acc_1))\n",
    "    print('Validation set 1: {:.2f}'.format(scores_1.mean()))\n",
    "    print('\\n')\n",
    "    # Target 5 days\n",
    "    rm_2 = RandomForestClassifier(max_depth=i)\n",
    "    rm_2.fit(X_train_2, Y_train_2)\n",
    "    train_acc_2 = accuracy_score(y_true= Y_train_2, y_pred= rm_2.predict(X_train_2))\n",
    "    scores_2 = cross_val_score(rm_2, X_train_2_80, Y_train_2_80, \n",
    "                             cv=5, scoring='accuracy', \n",
    "                             verbose = 0)\n",
    "    print(\"Train set 2: {:.2f}\".format(train_acc_2))\n",
    "    print('Validation set 2: {:.2f}'.format(scores_2.mean()))\n",
    "    print('\\n')\n",
    "    # Target 30 days\n",
    "    rm_3 = RandomForestClassifier(max_depth=i)\n",
    "    rm_3.fit(X_train_3, Y_train_3)\n",
    "    train_acc_3 = accuracy_score(y_true= Y_train_3, y_pred= rm_3.predict(X_train_3))\n",
    "    scores_3 = cross_val_score(rm_3, X_train_3_80, Y_train_3_80, \n",
    "                             cv=5, scoring='accuracy', \n",
    "                             verbose = 0)\n",
    "    print(\"Train set 3: {:.2f}\".format(train_acc_3))\n",
    "    print('Validation set 3: {:.2f}'.format(scores_3.mean()))\n",
    "    print('\\n')\n",
    "    \n",
    "    best_i.append([i, scores_1.mean() + scores_2.mean() + scores_3.mean()])\n",
    "    \n",
    "    \n",
    "i = max(best_i, key=lambda x:x[1])[0]\n",
    "print('Best max depth: ' + str(i) + '\\n')\n",
    "# Target 1 day\n",
    "rm_1 = RandomForestClassifier(max_depth=i)\n",
    "rm_1.fit(X_train_1_80, Y_train_1_80)\n",
    "test_acc_1 = accuracy_score(y_true= Y_test_1, y_pred= rm_1.predict(X_test_1))\n",
    "print('Test set 1: {:.2f}'.format(test_acc_1))\n",
    "\n",
    "# Target 5 days\n",
    "rm_2 = RandomForestClassifier(max_depth=i)\n",
    "rm_2.fit(X_train_2_80, Y_train_2_80)\n",
    "test_acc_2 = accuracy_score(y_true= Y_test_2, y_pred= rm_2.predict(X_test_2))\n",
    "print('Test set 2: {:.2f}'.format(test_acc_2))\n",
    "\n",
    "# Target 30 days\n",
    "rm_3 = RandomForestClassifier(max_depth=i)\n",
    "rm_3.fit(X_train_3_80, Y_train_3_80)\n",
    "test_acc_3 = accuracy_score(y_true= Y_test_3, y_pred= rm_3.predict(X_test_3))\n",
    "print('Test set 3: {:.2f}'.format(test_acc_3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T16:57:13.363953Z",
     "start_time": "2023-11-23T16:57:05.918768Z"
    }
   },
   "id": "f2d1549d2924d5bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Networks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b00188bdb25c44d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T16:57:13.364949Z"
    }
   },
   "id": "596d1f30215c374"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Train 1\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train_1, Y_train_1)\n",
    "Y_pred_1 = clf.predict(X_test_1)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(clf.score(X_test_1, Y_test_1)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T16:57:13.395818Z",
     "start_time": "2023-11-23T16:57:13.371327Z"
    }
   },
   "id": "15f68ab13049667e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Train 2\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train_2, Y_train_2)\n",
    "Y_pred_2 = clf.predict(X_test_2)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(clf.score(X_test_2, Y_test_2)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T16:57:13.379185Z"
    }
   },
   "id": "4fccd955ddc82557"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Train 3\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train_3, Y_train_3)\n",
    "Y_pred_3 = clf.predict(X_test_3)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(clf.score(X_test_3, Y_test_3)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T16:57:13.384309Z"
    }
   },
   "id": "eec02ec6db61406d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_i = []\n",
    "for i in [100,500,1000]:\n",
    "    print('Max iter: ' + str(i) + '\\n')\n",
    "    for j in [1,7,10,20]:\n",
    "        print('Random state: ' + str(j) + '\\n')\n",
    "        # Target 1 day\n",
    "        ann = MLPClassifier(random_state=j, max_iter=i)\n",
    "        ann.fit(X_train_1, Y_train_1)\n",
    "        train_acc_1 = accuracy_score(y_true= Y_train_1, y_pred= ann.predict(X_train_1))\n",
    "        scores_1 = cross_val_score(ann, X_train_1_80, Y_train_1_80, \n",
    "                                 cv=5, scoring='accuracy', \n",
    "                                 verbose = 0)\n",
    "        print(\"Train set 1: {:.2f}\".format(train_acc_1))\n",
    "        print('Validation set 1: {:.2f}'.format(scores_1.mean()))\n",
    "        print('\\n')\n",
    "        # Target 5 days\n",
    "        ann = MLPClassifier(random_state=j, max_iter=i)\n",
    "        ann.fit(X_train_2, Y_train_2)\n",
    "        train_acc_2 = accuracy_score(y_true= Y_train_2, y_pred= ann.predict(X_train_2))\n",
    "        scores_2 = cross_val_score(ann, X_train_2_80, Y_train_2_80, \n",
    "                                 cv=5, scoring='accuracy', \n",
    "                                 verbose = 0)\n",
    "        print(\"Train set 2: {:.2f}\".format(train_acc_2))\n",
    "        print('Validation set 2: {:.2f}'.format(scores_2.mean()))\n",
    "        print('\\n')\n",
    "        # Target 30 days\n",
    "        ann = MLPClassifier(random_state=j, max_iter=i)\n",
    "        ann.fit(X_train_3, Y_train_3)\n",
    "        train_acc_3 = accuracy_score(y_true= Y_train_3, y_pred= ann.predict(X_train_3))\n",
    "        scores_3 = cross_val_score(ann, X_train_3_80, Y_train_3_80, \n",
    "                                 cv=5, scoring='accuracy', \n",
    "                                 verbose = 0)\n",
    "        print(\"Train set 3: {:.2f}\".format(train_acc_3))\n",
    "        print('Validation set 3: {:.2f}'.format(scores_3.mean()))\n",
    "        print('\\n')\n",
    "        best_i.append([i, j, scores_1.mean() + scores_2.mean() + scores_3.mean()])\n",
    "        \n",
    "i = max(best_i, key=lambda x:x[2])[0]\n",
    "j = max(best_i, key=lambda x:x[2])[1]\n",
    "print('Best max iter: ' + str(i))\n",
    "print('Best random state: ' + str(j))\n",
    "\n",
    "# Target 1 day\n",
    "ann = MLPClassifier(random_state=j, max_iter=i)\n",
    "ann.fit(X_train_1_80, Y_train_1_80)\n",
    "test_acc_1 = accuracy_score(y_true= Y_test_1, y_pred= ann.predict(X_test_1))\n",
    "print('Test set 1: {:.2f}'.format(test_acc_1))\n",
    "\n",
    "# Target 5 days\n",
    "ann = MLPClassifier(random_state=j, max_iter=i)\n",
    "ann.fit(X_train_2_80, Y_train_2_80)\n",
    "test_acc_2 = accuracy_score(y_true= Y_test_2, y_pred= ann.predict(X_test_2))\n",
    "print('Test set 2: {:.2f}'.format(test_acc_2))\n",
    "\n",
    "# Target 30 days\n",
    "ann = MLPClassifier(random_state=j, max_iter=i)\n",
    "ann.fit(X_train_3_80, Y_train_3_80)\n",
    "test_acc_3 = accuracy_score(y_true= Y_test_3, y_pred= ann.predict(X_test_3))\n",
    "print('Test set 3: {:.2f}'.format(test_acc_3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T16:57:13.388883Z"
    }
   },
   "id": "70f38e1723ec5e6d"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a7b86753ed2ec1fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
