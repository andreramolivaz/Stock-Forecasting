{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba14fa86b7707f83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "Reinforcement Learning is a branch of machine learning that focuses on how agents should act in an environment to maximize a notion of cumulative reward. Unlike supervised learning, where algorithms are trained on labeled examples, RL learns from a trial-and-error process based on the environment's responses to its actions.\n",
    "\n",
    "Differences between RL and other learning methods:\n",
    "\n",
    "- **Supervised learning**: Algorithms learn from labeled examples, trying to predict label based on the training set;\n",
    "- **Unsupervised learning**: Algorithms try to find patterns in the data, without any labels;\n",
    "- **Reinforcement learning**: Agent learns to make decision through rewards (or punishments) received from its actions.\n",
    "\n",
    "**Key elements:**\n",
    "\n",
    "- Agent:\n",
    "\n",
    "    - **Definition**: The agent is the entity that makes decisions and learns through interaction with the environment. In RL, the agent chooses actions to take based on its current policy.\n",
    "    - **Role in RL**: The agent is at the center of learning in RL. Its goal is to learn the best possible policy, that is, a map from states to actions, to maximize the total reward collected over time. \n",
    "\n",
    "- Enviroment:\n",
    "\n",
    "    - **Definition**: Environment represents the context or world in which the agent operates. It includes anything that the agent can interact with but does not have direct control over;\n",
    "    - **Interaction with the agent**: The environment responds to the agent's actions and presents new states and rewards to the agent. The nature of the environment can vary from simple and static to complex and dynamic.\n",
    "\n",
    "- State:\n",
    "\n",
    "    - **Definition**: A state is a configuration or representation of the environment at a given time. States provide the information the agent needs to make decisions.\n",
    "    - **Importance**: The quality and quantity of information available in states can significantly influence the effectiveness of agent learning.\n",
    "\n",
    "- Action:\n",
    "\n",
    "    - **Definition**: Actions are the various behaviors or moves that the agent can perform. The set of all available actions is known as the action space.\n",
    "    - **Action-Stata dynamics**: Every action taken by the agent affects the state of the environment. The relationship between actions and their consequence on states is fundamental to the agent's decision making. \n",
    "\n",
    "- Reward:\n",
    "\n",
    "    - **Definnition**: A reward is immediate feedback provided to the agent by the environment as a consequence of his or her actions. Rewards can be positive (to encourage certain actions) or negative (to discourage certain actions).\n",
    "    - **Role in learning**: Rewards are the main guide for the agent in the learning process. The agent's goal is to maximize the sum of rewards over time, often referred to as return.\n",
    "\n",
    "- Policy:\n",
    "\n",
    "    - **Definition**: A policy is a strategy adopted by the agent, a kind of rule or algorithm that decides what action to take based on the current state.\n",
    "    - **Types**: Policies can be deterministic or stochastic. A deterministic policy always provides the same action for a given state, while a stochastic policy selects actions according to a probability distribution.\n",
    "\n",
    "- Evaluation functions:\n",
    "\n",
    "    - **Goal**: These functions help the agent evaluate the effectiveness of his or her actions and policies.\n",
    "    - **Types**:\n",
    "        - **Value function**: Estimates the expected return from a state following a given policy.\n",
    "        - **Q-Value function**: also known as **Action-Value function**. Estimates the expected return for a state-action pair.\n",
    "\n",
    "- Model:\n",
    "\n",
    "    - **Description**: A model is an internal representation of the environment that the agent uses to predict how the environment will respond to its actions.\n",
    "    - **Model-based vs model-free RL**: In model-based RL, the agent uses an explicit model of the environment to plan its actions. In model-free RL, the agent learns directly from interactions with the environment without an explicit model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "311b6c11919b8eb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Theoretical concepts\n",
    "\n",
    "#### 1. Markov Decision Process\n",
    "\n",
    "The Markov Decision Process (MDP) is a fundamental mathematical framework in the field of Reinforcement Learning. It provides a formalization for decision-making in uncertain and dynamic situations. An MDP is characterized by a set of states, a set of actions, transition probabilities and reward functions.\n",
    "\n",
    "Key MDP elements:\n",
    "\n",
    "- **States**: A set of states $S$ represents all the possible configurations the environment can be in.\n",
    "- **Actions**: A set of actions $A$ that the agent can take. The set of available actions may depend on the current state.\n",
    "- **Transition probability**: A transaction function $P(s_{t+1} | s_t, a_t)$ which defines the probability of transition to the state $s_{t+1}$ given the current state $s_t$ and action $a_t$.\n",
    "- **Reward**: A reward function $R(s_t, a_t, s_{t+1})$ which assigns a reward (or a punishment) to the agent for the transition from state $s_t$ to state $s_{t+1}$ after taking action $a_t$.\n",
    "\n",
    "The fundamental property of an MDP is the \"Markov property,\" which states that the future is independent of the past given the present. This means that the transition probability and reward depend only on the current state and the action taken, not on the history of previous actions or states.\n",
    "\n",
    "#### 2. Reward and evaluation function\n",
    " \n",
    "In the context of Reinforcement Learning, the reward and evaluation function are central concepts that guide agent learning and decision-making. This chapter explores the nature of these components and their role in RL.\n",
    "\n",
    "Reward:\n",
    "\n",
    "- **Definition**: A reward is a scalar value that represents the immediate feedback provided to the agent by the environment as a consequence of his or her actions. Rewards can be positive (to encourage certain actions) or negative (to discourage certain actions).\n",
    "- **Role in learning**: Rewards are the main guide for the agent in the learning process. The agent's goal is to maximize the sum of rewards over time, often referred to as return.\n",
    "\n",
    "Evaluation function:\n",
    " \n",
    "- **Value function**: The value function, denoted as $V(s)$, estimates the expected total return from a states following a given policy. It provides a measure of the goodness-of-fit of a state.\n",
    "- **Q-value function**: The Q-value function, or action-value function, denoted as $Q(s, a)$, evaluates the action $a$ in the state $s$. It estimates the expected return following the action $a$ in the state $s$ and then adhering to a specific policy. \n",
    "\n",
    "#### 3. Common RL algorithms\n",
    "\n",
    "- Q-learning is a model-free learning method that seeks to learn an optimal policy independently of the agent's current action.\n",
    "The algorithm iteratively updates the Q-value estimates for each state-action pair using the Q-learning update formula, based on the reward received and the maximum Q-value of the next state. Q-learning is widely used for problems with discrete state and action spaces and is well suited to situations with uncertain environmental dynamics.\n",
    "\n",
    "\n",
    "- SARSA is a model-based differential time learning (TD) algorithm.\n",
    "Unlike Q-learning, SARSA updates its Q-values based on the agent's current policy (on-policy). The update considers the current transition and the next action the agent intends to perform. SARSA is useful in environments where risk assessment and safety considerations are important because it takes into account the actual path the agent plans to follow.\n",
    "\n",
    "Some remarks:\n",
    "- **Exploration vs. exploitation**: A key aspect in RL is the balance between exploration (trying new actions) and exploitation (using acquired knowledge). RL algorithms must effectively manage this balance;\n",
    "- **Scalability and complexity**: the scalability of algorithms in environments with large state and action spaces is a significant challenge. Methods such as deep learning have been integrated into RL to address this challenge."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55430e14daeb171f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use of RL in finance\n",
    "\n",
    "In trading, RL can be used to develop automated strategies that decide when to buy, sell, or hold a stock or cryptocurrency.\n",
    "Financial markets are complex, noisy, and nonstationary, making trading an ideal challenge for RL, which can adapt to such dynamic conditions.\n",
    "\n",
    "Challenges:\n",
    "\n",
    "- Financial data are often noisy and exhibit nonstationarity, which can lead to overfitting and inconsistent model performance.\n",
    "- Markets change rapidly, and what has worked in the past may no longer be valid, requiring the model to continually adapt.\n",
    "\n",
    "Pros of using RL:\n",
    "\n",
    "- **Automation and Scalability**: The RL can automate trading decisions and operate on a large scale, analyzing huge amounts of data more efficiently than human analysis.\n",
    "- **Adaptability**: RL models can dynamically adapt to changes in the market, continuously learning from new data.\n",
    "- **High Performance Potential**: When configured well, RL models can potentially outperform traditional strategies and human traders, especially in highly volatile markets.\n",
    "\n",
    "Common approaches:\n",
    "\n",
    "- **Deep reinforcement learning**: The use of deep neural networks to handle the complexity of market data and capture nonlinear relationships.\n",
    "- **Optimized exploration strategies**: Development of exploration methods that balance between learning from historical market situations and exploring new trading strategies.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a10463acab5730d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Practical Implementation\n",
    "\n",
    "OpenAI Gym is an open-source library developed by OpenAI that provides a set of environments for developing and comparing Reinforcement Learning algorithms. Gym is designed to make access to and implementation of RL environments simple and standardized. \n",
    "\n",
    "Features of Gym:\n",
    "- **Diversified Environments:** Gym offers a wide range of environments, from simple control problems to complex physics- and pixel-based environments.\n",
    "- **Standardized Interface:** Provides a consistent API for interacting with environments, making it easier to test different algorithms.\n",
    "- **Flexibility:** Gym is compatible with several machine learning frameworks and libraries, allowing integration with advanced RL algorithms.\n",
    "\n",
    "\"TradingEnv\" is a Gym-specific environment that simulates the stock or cryptocurrency market for trading. This environment provides an ideal platform for testing and evaluating RL-based trading strategies. The main features of \"TradingEnv\" are:\n",
    "\n",
    "- **Market Simulation:** Reproduces stock or cryptocurrency price movements, providing the agent with realistic data on which to act.\n",
    "- **Actions and Trading Decisions:** The agent can execute actions such as buy, sell or hold, based on current market information.\n",
    "- **Feedback and Rewards:** The environment provides feedback in terms of rewards or penalties based on the agent's trading performance.\n",
    "\n",
    "Considerations:\n",
    "- **Realism and Limitations**: although \"TradingEnv\" offers a realistic simulation, it is important to recognize its limitations and the difference between a simulated environment and real trading.\n",
    "- **Parameter Sensitivity**: the effectiveness of the RL agent may be sensitive to the parameters of the environment, including simulated pricing patterns and market conditions.\n",
    "- **Testing and Evaluation**: it is crucial to perform extensive testing and model evaluations to ensure that the learned strategies are robust and reliable before considering their application in real trading situations.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b4194683da6b689"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Understanding action space\n",
    "#### Positions\n",
    "The agent can take two positions:\n",
    "- `1`: convert the whole of the portfolio into BTC\n",
    "- `0`: the portfolio is converted into USD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "942a3345a801f323"
  },
  {
   "cell_type": "markdown",
   "source": [
    "ref: [https://gym-trading-env.readthedocs.io/en/latest/index.html](https://gym-trading-env.readthedocs.io/en/latest/index.html)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c8d203a849064d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Import dataset\n",
    "In order for this enviroment to work, the dataset must be in the following format:\n",
    "- it must be ordered by **ascending date**\n",
    "- the index must be a **DatetimeIndex**\n",
    "- it must have **Close**, **Open**, **High**, **Low**, **Volume** labels at least"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1dc5eca9cfe7be1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T17:06:53.056650Z",
     "start_time": "2023-12-27T17:06:53.043116Z"
    }
   },
   "id": "e22504fc10c479d7"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "dataset = './datasets/BTC_USD-Hourly.csv'\n",
    "df = pd.read_csv(dataset, parse_dates = ['date'], index_col = 'date')\n",
    "df.sort_index(inplace = True)\n",
    "df.dropna(inplace = True)\n",
    "df.drop_duplicates(inplace = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T17:06:53.265288Z",
     "start_time": "2023-12-27T17:06:53.046757Z"
    }
   },
   "id": "f0e4495a4db89d5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adding static features\n",
    "The reinforcement learning agent will need inputs.\n",
    "This enviroment treats as input every column that has **feature** in its name."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "209f56b2a746b059"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df['feature_close'] = df['close'].pct_change()\n",
    "df['feature_open'] = df['open'] / df['close']\n",
    "df['feature_high'] = df['high'] / df['close']\n",
    "df['feature_low'] = df['low'] / df['close']\n",
    "df['feature_volume'] = df['Volume USD'] / df['Volume USD'].rolling(7 * 24).max()\n",
    "\n",
    "df.dropna(inplace = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T17:06:53.278623Z",
     "start_time": "2023-12-27T17:06:53.269732Z"
    }
   },
   "id": "cba88f7e68bd0c13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above features are called **static features**; this means they are computed once and they are not updated at each step.\n",
    "We'd also need **dynamic features**, which are computed at each step."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4c2fbc2bca3cc33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adding dynamic features\n",
    "A **dynamic feature** is computed at each step, that's why we need to be careful: dynamic features can be *computationally more expensive* than static features.\n",
    "The dynamic features below are the default dynamic features of the enviroment."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb395068d656fdc9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def dynamic_feature_last_position_taken(history):\n",
    "    return history['position', -1]\n",
    "\n",
    "def dynamic_feature_real_position(history):\n",
    "    return history['real_position', -1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T17:06:53.282554Z",
     "start_time": "2023-12-27T17:06:53.279096Z"
    }
   },
   "id": "6e2a4c487141cdf6",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating the enviroment\n",
    "Let's examine the parameters of the enviroment:\n",
    "- **name**: the name of the enviroment\n",
    "- **df**: the dataframe\n",
    "- **positions**: list of the positions allowed by the enviroment\n",
    "- **trading_fees**: the trading fees (buy and sell operations)\n",
    "- **borrow_interest_rate**: the interest rate for borrowing money"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f10a43370ae9fed"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "env = gym.make(\n",
    "    'TradingEnv',\n",
    "    name = 'BTCUSD',\n",
    "    df = df,\n",
    "    dynamic_feature_functions = [\n",
    "        dynamic_feature_last_position_taken,\n",
    "        dynamic_feature_real_position\n",
    "    ],\n",
    "    positions = [-1, 0, 1],\n",
    "    trading_fees = 0.01 / 100,\n",
    "    borrow_interest_rate = 0.003 / 100\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T17:06:53.304513Z",
     "start_time": "2023-12-27T17:06:53.283794Z"
    }
   },
   "id": "3b0ed866e1c06443"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run the enviroment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6161a888919d695"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 423.10%   |   Portfolio Return : -99.12%   |   \n"
     ]
    }
   ],
   "source": [
    "done, truncated = False, False\n",
    "observation, info = env.reset()\n",
    "while not done and not truncated:\n",
    "    position_index = env.action_space.sample()\n",
    "    observation, reward, done, truncated, info = env.step(position_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T17:06:53.829324Z",
     "start_time": "2023-12-27T17:06:53.303417Z"
    }
   },
   "id": "c896f4450c9381aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
