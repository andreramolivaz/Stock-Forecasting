{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Libraries\n",
    "In this section, we are initializing our environment with three fundamental Python libraries that will facilitate data analysis, numerical operations, and file management. Here's a brief overview of each:\n",
    "\n",
    "1. **pandas (`pd`)**: \n",
    "    - **Description**: pandas is a powerful library for data manipulation and analysis. It offers data structures like DataFrames and Series that simplify handling structured data.\n",
    "    - **Usage**: Throughout this notebook, we'll utilize `pd` for tasks like data ingestion, cleaning, transformation, and aggregation.\n",
    "\n",
    "2. **numpy (`np`)**:\n",
    "    - **Description**: numpy is the core library for numerical computing in Python. It provides support for working with arrays (including mathematical functions to operate on these arrays), which is essential for various data science tasks.\n",
    "    - **Usage**: We'll employ `np` whenever we need to perform mathematical and statistical operations, especially when dealing with arrays or matrices of data.\n",
    "\n",
    "3. **os**:\n",
    "    - **Description**: The os module provides a way of using operating system-dependent functionality, such as reading or writing to the file system.\n",
    "    - **Usage**: In this notebook, `os` will be instrumental in tasks related to file management, such as directory creation, file path handling, and checking the presence of files.\n",
    "\n",
    "By importing these libraries, we're equipping our notebook with the tools necessary to handle a wide range of data analysis and computational tasks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:46:17.988682Z",
     "start_time": "2023-11-02T15:46:17.242254Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting Up Data Retrieval Infrastructure\n",
    "This code segment establishes the foundation for managing and processing our financial data. It can be broken down into two main sections:\n",
    "1. **Predefined Sheet Names**:\n",
    "    - **Description**: We define a list called `sheet_names` that contains the names of sheets expected in the Excel files. Each of these sheets corresponds to a specific type of financial data.\n",
    "    - **Usage**: This predefined list will be instrumental when reading and processing data from Excel files, ensuring that we access the correct sheets and maintain consistency in data handling.\n",
    "\n",
    "2. **Retrieving File Paths for Data Processing:**\n",
    "    - **Description**: The above code sets the path to the directory where all the Excel files (with financial data) are stored. It then constructs a list called `stocks`, containing the full paths to each of these files.\n",
    "    - **Usage**: With this setup, we'll have easy access to all the Excel files in the `data` directory, facilitating bulk processing and analysis of the financial data contained in them.\n",
    "\n",
    "In essence, this segment prepares our environment to efficiently load, access, and analyze the financial datasets saved in the Excel format."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:46:45.707017Z",
     "start_time": "2023-11-02T15:46:45.694095Z"
    }
   },
   "outputs": [],
   "source": [
    "sheet_names = [\n",
    "    'Info',\n",
    "    'Historical',\n",
    "    'Income Statement',\n",
    "    'Quarterly Income Statement',\n",
    "    'Cashflow',\n",
    "    'Institutional Holders',\n",
    "    'Mutual Fund Holders',\n",
    "    'Major Holders'\n",
    "]\n",
    "\n",
    "#riempire stocks di tutti i vari codici, fare la retrive di tutti i file e buttarli in df_stock per poi poter lavorare su tutti i dati\n",
    "directory = \"./data\"\n",
    "stocks = [os.path.join(directory, file) for file in os.listdir(directory)]\n",
    "stocks = [stock.replace(\"\\\\\", \"/\") for stock in stocks] # lambda per unificare i percorsi tra mac e windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Integration of Financial Data\n",
    "Added columns:\n",
    "- **Daily_Return**: Represents the daily return.\n",
    "  \n",
    "- **Target_1day**: Indicates whether the closing price the next day will be higher (1) or lower (0) than the current day's closing price.\n",
    "\n",
    "- **Target_5days**: Specifies if the closing price 5 days into the future will be higher (1) or lower (0) than the closing price of the current day.\n",
    "\n",
    "- **Target_30days**: Determines if the closing price 30 days into the future will be higher (1) or lower (0) compared to the current day's closing price.\n",
    "\n",
    "- **Net income** (Income statement): This metric calculates a company's profit after all expenses and taxes have been deducted. It stands as the most critical measure for investors, showcasing the company's bottom line.\n",
    "\n",
    "- **Diluted EPS** (Income statement): Measures the company's profit for each share of common stock, offering a clear picture of the company's profitability per share.\n",
    "\n",
    "- **Total Revenue** (Income statement): Reflects the total sales a company achieves. It provides a snapshot of the company's top line growth.\n",
    "\n",
    "- **Cost of revenue** (Income statement): Calculates the cost of the goods the company sells, an essential measure for gauging a company's profitability.\n",
    "\n",
    "- **Operating revenue** (Income statement): Captures the non-production costs incurred by a company. It's vital to factor in operating expenses when assessing a company's profitability and cash flow potential.\n",
    "\n",
    "- **Cash flow from continuing operating activities** (Cashflow): Represents the cash generated by a company from its primary business ventures.\n",
    "\n",
    "- **Cash flow from continuing investing activities** (Cashflow): Indicates the cash a company earns from its investments, such as selling property or equipment.\n",
    "\n",
    "- **Cash flow from continuing financing activities** (Cashflow): Reflects the cash a company secures from its financing activities, like issuing debt or equity.\n",
    "\n",
    "We've merged the \"Income Statement\" and \"Cashflow\" sheets into one Excel document. Note: As these sheets contain yearly or quarterly financial data, a common approach is to carry forward the last known value for each day until new data becomes available. For certain fiscal years, the value will be NaN as we don't possess the data.\n",
    "\n",
    "The dataframes for each stock were truncated at 06/30/2020 and were also individually exported for potential future use. Lastly, we combined all stocks into a singular final dataset, identifying each using the \"Ticker\" feature we've added.\n",
    "\n",
    "Some companies lacked complete financial data for our analysis. Therefore, by mutual agreement, we chose to still consider such data, filling in the missing features with neutral values (0). This enables us to include these data points and add value to the analysis of companies whose data is available.\n",
    "\n",
    "## Feature Engineering\n",
    "- **Moving Averages**: We calculate short and long-term moving averages for the closing price, which are standard in algorithmic trading. For instance, moving averages over 5, 10, 30, and 50 days.\n",
    "  \n",
    "- **RSI (Relative Strength Index)**: This is a momentum indicator that can help identify if a stock is in an \"overbought\" or \"oversold\" condition.\n",
    "\n",
    "- **MACD (Moving Average Convergence Divergence)**: Another momentum indicator.\n",
    "\n",
    "- **Bollinger Bands**: These are based on moving averages and can help determine if a price is relatively high or low.\n",
    "\n",
    "- **Volatility**: We might compute volatility as the standard deviation of daily returns over a specific timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Financial Data Integration and Feature Engineering\n",
    "\n",
    "This code block is designed to process financial data for multiple stocks, adding derived features, and integrating various sheets from Excel files into a unified DataFrame structure. Here's a step-by-step explanation:\n",
    "\n",
    "1. **Initialization**:\n",
    "    - `counter` is set to 0, which will be used later in the code.\n",
    "    - `df` is an empty DataFrame that will store the consolidated data from all processed stocks.\n",
    "\n",
    "2. **Iterating through Stock Files**:\n",
    "    The loop goes through each file path in the `stocks` list, representing different stocks' data.\n",
    "\n",
    "3. **Loading Excel Data**:\n",
    "    The code reads an Excel file corresponding to a specific stock into the `df_stock` DataFrame.\n",
    "\n",
    "4. **Processing 'Historical' Data**:\n",
    "    - The 'Historical' sheet is loaded.\n",
    "    - The date column is renamed and set as the index.\n",
    "    - The 'Daily_Return' column is calculated to represent the daily percentage change in the closing price.\n",
    "    - Three target columns (`Target_1day`, `Target_5days`, and `Target_30days`) are added to indicate whether the closing price in the future (1, 5, or 30 days respectively) will be higher than the current day's closing price.\n",
    "    - Rows with NaN values (resulting from the target creation process) are dropped.\n",
    "\n",
    "5. **Processing 'Income Statement' Data**:\n",
    "    - The 'Income Statement' sheet is loaded and transposed for easier integration.\n",
    "    - Key financial metrics (like 'Net Income', 'Diluted EPS', etc.) are selected.\n",
    "    - Missing columns for the selected metrics are created and filled with NaN values.\n",
    "    - The data is then merged with the historical stock data.\n",
    "\n",
    "6. **Processing 'Cashflow' Data**:\n",
    "    - Similar to the 'Income Statement' data, the 'Cashflow' sheet is loaded, transposed, and key metrics are selected.\n",
    "    - The cashflow data is then merged with the existing DataFrame.\n",
    "\n",
    "7. **Feature Engineering**:\n",
    "    The following technical indicators and financial metrics are calculated and added to the DataFrame:\n",
    "    - **Moving Averages**: Calculated for the closing prices over 5, 10, 30, and 50 days.\n",
    "    - **RSI (Relative Strength Index)**: A momentum indicator.\n",
    "    - **MACD**: Another momentum indicator with a corresponding signal line.\n",
    "    - **Bollinger Bands**: Used to determine relative high or low prices.\n",
    "    - **Volatility**: Calculated as the standard deviation of daily returns over a specified window.\n",
    "\n",
    "8. **Final Processing**:\n",
    "    - A 'Ticker' column is added to the DataFrame to store the stock's identifier.\n",
    "    - NaN values in various columns are filled with neutral values (0).\n",
    "    - The data is truncated to only include entries after '2020-06-30'.\n",
    "    - Each processed stock's data is saved in the 'Processed' directory.\n",
    "    - The processed stock's data is then appended to the consolidated DataFrame `df`.\n",
    "\n",
    "In essence, this code processes financial data for multiple stocks, derives meaningful metrics, integrates different data sheets, and prepares the data for further analysis or modeling tasks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:46:55.754466Z",
     "start_time": "2023-11-02T15:46:52.505702Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7l/9mpg_4kx7_zbbl179wmh30sc0000gn/T/ipykernel_4752/996895617.py:59: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data[selected_metrics] = merged_data[selected_metrics].fillna(method='ffill')\n",
      "/var/folders/7l/9mpg_4kx7_zbbl179wmh30sc0000gn/T/ipykernel_4752/996895617.py:88: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data[selected_cashflow_metrics] = merged_data[selected_cashflow_metrics].fillna(method='ffill')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Worksheet named 'Income Statement' not found",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 30\u001B[0m\n\u001B[1;32m     27\u001B[0m historical_data \u001B[38;5;241m=\u001B[39m historical_data\u001B[38;5;241m.\u001B[39mdropna()\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# Loading the 'Income Statement' data for XOM\u001B[39;00m\n\u001B[0;32m---> 30\u001B[0m income_statement \u001B[38;5;241m=\u001B[39m \u001B[43mdf_stock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mIncome Statement\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Transposing the data for easier integration\u001B[39;00m\n\u001B[1;32m     33\u001B[0m income_statement \u001B[38;5;241m=\u001B[39m income_statement\u001B[38;5;241m.\u001B[39mset_index(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnnamed: 0\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mtranspose()\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/pandas/io/excel/_base.py:1629\u001B[0m, in \u001B[0;36mExcelFile.parse\u001B[0;34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001B[0m\n\u001B[1;32m   1589\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparse\u001B[39m(\n\u001B[1;32m   1590\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1591\u001B[0m     sheet_name: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1609\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds,\n\u001B[1;32m   1610\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, DataFrame] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mint\u001B[39m, DataFrame]:\n\u001B[1;32m   1611\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1612\u001B[0m \u001B[38;5;124;03m    Parse specified sheet(s) into a DataFrame.\u001B[39;00m\n\u001B[1;32m   1613\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1627\u001B[0m \u001B[38;5;124;03m    >>> file.parse()  # doctest: +SKIP\u001B[39;00m\n\u001B[1;32m   1628\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1629\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1630\u001B[0m \u001B[43m        \u001B[49m\u001B[43msheet_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msheet_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1631\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1632\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1633\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_col\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1634\u001B[0m \u001B[43m        \u001B[49m\u001B[43musecols\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43musecols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1635\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconverters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconverters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1636\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrue_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrue_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1637\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfalse_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfalse_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1638\u001B[0m \u001B[43m        \u001B[49m\u001B[43mskiprows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskiprows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1639\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnrows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1640\u001B[0m \u001B[43m        \u001B[49m\u001B[43mna_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1641\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparse_dates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_dates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1642\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdate_parser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_parser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1643\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1644\u001B[0m \u001B[43m        \u001B[49m\u001B[43mthousands\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthousands\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1645\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcomment\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcomment\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1646\u001B[0m \u001B[43m        \u001B[49m\u001B[43mskipfooter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskipfooter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1647\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype_backend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype_backend\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1648\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1649\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/pandas/io/excel/_base.py:788\u001B[0m, in \u001B[0;36mBaseExcelReader.parse\u001B[0;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001B[0m\n\u001B[1;32m    785\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReading sheet \u001B[39m\u001B[38;5;132;01m{\u001B[39;00masheetname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    787\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(asheetname, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 788\u001B[0m     sheet \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_sheet_by_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43masheetname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    789\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# assume an integer if not a string\u001B[39;00m\n\u001B[1;32m    790\u001B[0m     sheet \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_sheet_by_index(asheetname)\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/pandas/io/excel/_openpyxl.py:583\u001B[0m, in \u001B[0;36mOpenpyxlReader.get_sheet_by_name\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_sheet_by_name\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 583\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_if_bad_sheet_by_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    584\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbook[name]\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/pandas/io/excel/_base.py:639\u001B[0m, in \u001B[0;36mBaseExcelReader.raise_if_bad_sheet_by_name\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    637\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mraise_if_bad_sheet_by_name\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    638\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msheet_names:\n\u001B[0;32m--> 639\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWorksheet named \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m not found\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Worksheet named 'Income Statement' not found"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "df = pd.DataFrame()\n",
    "for file in stocks:\n",
    "    df_stock = pd.ExcelFile(file)\n",
    "    \n",
    "    # prevent false postive warnings, reference_ https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas\n",
    "    pd.options.mode.chained_assignment = None # default='warn'\n",
    "    \n",
    "    # Loading the 'Historical' data stock\n",
    "    historical_data = df_stock.parse('Historical')\n",
    "    \n",
    "    # Renaming and setting the Date column\n",
    "    historical_data.rename(columns={'Unnamed: 0': 'Date'}, inplace=True)\n",
    "    historical_data['Date'] = pd.to_datetime(historical_data['Date'])\n",
    "    historical_data.set_index('Date', inplace=True)\n",
    "    # Calculate daily return\n",
    "    historical_data['Daily_Return'] = historical_data['Open'] - historical_data['Close']\n",
    "    historical_data['Daily_Return_Percentage'] = (historical_data['Open'] * historical_data['Daily_Return']) / 100\n",
    "    \n",
    "    \n",
    "    # Create target variables for next day, next 5 days and next 30 days\n",
    "    historical_data['Target_1day'] = (historical_data['Close'].shift(-1) > historical_data['Close']).astype(int)\n",
    "    historical_data['Target_5days'] = (historical_data['Close'].shift(-5) > historical_data['Close']).astype(int)\n",
    "    historical_data['Target_30days'] = (historical_data['Close'].shift(-30) > historical_data['Close']).astype(int)\n",
    "    \n",
    "    # Drop rows with NaN values (will be present due to the shifting for target creation)\n",
    "    historical_data = historical_data.dropna()\n",
    "    \n",
    "    income_statement = df_stock.parse('Income Statement')\n",
    "    \n",
    "    # Transposing the data for easier integration\n",
    "    income_statement = income_statement.set_index('Unnamed: 0').transpose()\n",
    "    income_statement.index = pd.to_datetime(income_statement.index)\n",
    "    \n",
    "    \n",
    "    # Selecting some of the key financial metrics (you can add or remove based on relevance)\n",
    "    selected_metrics = [\n",
    "        'Net Income',\n",
    "        'Diluted EPS',\n",
    "        'Total Revenue',\n",
    "        'Normalized EBITDA',\n",
    "        'Total Unusual Items',\n",
    "        'Total Unusual Items Excluding Goodwill'\n",
    "    ]\n",
    "    \n",
    "    # check if columns exist, in case create them\n",
    "    for metric in selected_metrics:\n",
    "        if metric not in income_statement.columns:\n",
    "            income_statement[metric] = np.nan\n",
    "            \n",
    "    \n",
    "    income_statement = income_statement[selected_metrics]\n",
    "    \n",
    "    # Merging the income statement data with the historical data\n",
    "    merged_data = historical_data.join(income_statement, how='left')\n",
    "    \n",
    "    # Forward filling the NaN values\n",
    "    merged_data[selected_metrics] = merged_data[selected_metrics].fillna(method='ffill')\n",
    "    \n",
    "    cashflow = df_stock.parse('Cashflow')\n",
    "    \n",
    "    # Transposing the data for easier integration\n",
    "    cashflow = cashflow.set_index('Unnamed: 0').transpose()\n",
    "    cashflow.index = pd.to_datetime(cashflow.index)\n",
    "    \n",
    "    # Selecting some of the key cashflow metrics (you can add or remove based on relevance)\n",
    "    selected_cashflow_metrics = [\n",
    "        'Operating Cash Flow',\n",
    "        'Capital Expenditure',\n",
    "        'Free Cash Flow',\n",
    "        'Cash Flow From Continuing Operating Activities',\n",
    "        'Cash Flow From Continuing Investing Activities',\n",
    "        'Cash Flow From Continuing Financing Activities'\n",
    "    ]\n",
    "    \n",
    "    for metric in selected_cashflow_metrics:\n",
    "        if metric not in cashflow.columns:\n",
    "            cashflow[metric] = np.nan\n",
    "    \n",
    "    cashflow = cashflow[selected_cashflow_metrics]\n",
    "    \n",
    "    # Merging the cashflow data with the existing dataframe\n",
    "    merged_data = merged_data.join(cashflow, how='left', rsuffix='_cashflow')\n",
    "    \n",
    "    # Forward filling the NaN values\n",
    "    merged_data[selected_cashflow_metrics] = merged_data[selected_cashflow_metrics].fillna(method='ffill')\n",
    "\n",
    "    # Moving Averages\n",
    "    merged_data['MA_5'] = merged_data['Close'].rolling(window=5).mean()\n",
    "    merged_data['MA_10'] = merged_data['Close'].rolling(window=10).mean()\n",
    "    merged_data['MA_30'] = merged_data['Close'].rolling(window=30).mean()\n",
    "    merged_data['MA_50'] = merged_data['Close'].rolling(window=50).mean()\n",
    "    \n",
    "    # RSI\n",
    "    delta = merged_data['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).fillna(0)\n",
    "    loss = (-delta.where(delta < 0, 0)).fillna(0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    merged_data['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    merged_data['MACD'] = merged_data['Close'].ewm(span=12, adjust=False).mean() - merged_data['Close'].ewm(span=26, adjust=False).mean()\n",
    "    merged_data['Signal_Line'] = merged_data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    merged_data['Bollinger_Mid_Band'] = merged_data['Close'].rolling(window=20).mean()\n",
    "    merged_data['Bollinger_Upper_Band']  = merged_data['Bollinger_Mid_Band'] + 1.96*merged_data['Close'].rolling(window=20).std()\n",
    "    merged_data['Bollinger_Lower_Band']  = merged_data['Bollinger_Mid_Band'] - 1.96*merged_data['Close'].rolling(window=20).std()\n",
    "    \n",
    "    # Volatility\n",
    "    merged_data['Volatility'] = merged_data['Daily_Return'].rolling(window=5).std()\n",
    "    \n",
    "    to_drop_na = ['MA_5', 'MA_10', 'MA_30', 'MA_50', 'RSI', 'Volatility', 'Cash Flow From Continuing Operating Activities', 'Cash Flow From Continuing Investing Activities', 'Cash Flow From Continuing Financing Activities', 'Net Income', 'Diluted EPS', 'Total Revenue','Normalized EBITDA', 'Total Unusual Items', 'Total Unusual Items Excluding Goodwill', 'Operating Cash Flow', 'Capital Expenditure','Free Cash Flow',]\n",
    "\n",
    "    if 'Ticker' not in merged_data.columns:\n",
    "            merged_data['Ticker'] = file.split(\"/\")[2].replace(\".\", \"\")[:-4]\n",
    "        \n",
    "    # Display the updated dataframe with integrated cashflow metrics\n",
    "    merged_data.iloc[counter : counter + len(merged_data), merged_data.columns.get_loc(\"Ticker\")] = file.split(\"/\")[2].replace(\".\", \"\")[:-4]\n",
    "        \n",
    "    counter = len(merged_data)\n",
    "    \n",
    "    for column in to_drop_na:\n",
    "        merged_data[column] = merged_data[column].fillna(0)\n",
    "\n",
    "    merged_data = merged_data[merged_data.index >= '2020-06-30']\n",
    "    #indices_to_drop = merged_data.index[merged_data.isna().sum(axis=1) > 3].tolist()\n",
    "    \n",
    "    #merged_data.drop(indices_to_drop, inplace=True)\n",
    "    \n",
    "    # Export in Excel company data\n",
    "    if not os.path.exists('./Processed'):\n",
    "        os.makedirs('./Processed')\n",
    "    with pd.ExcelWriter(f'./Processed/{file.split(\"/\")[2][:-5]}.xlsx', mode = \"w\", engine = \"openpyxl\") as writer:\n",
    "        merged_data.to_excel(writer, sheet_name=\"Sheet1\")\n",
    "    # Append to one single dataframe\n",
    "    df = pd.concat([df, merged_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export del dataset finale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T19:35:32.903283100Z",
     "start_time": "2023-10-26T19:35:16.918414500Z"
    }
   },
   "outputs": [],
   "source": [
    "output_filepath = \"final_dataset.xlsx\"\n",
    "df.to_excel(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# TODO: Rifinire il markdown del blocco di codice che fa tutto e gli altri due.\n",
    "# TODO: Procedere al train del se il dataset viene considerato pronto.\n",
    "# TODO: Replicare con le crypto."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T19:35:32.912399200Z",
     "start_time": "2023-10-26T19:35:32.905884Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classifier similar rows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'finnhub'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfinnhub\u001B[39;00m\n\u001B[1;32m      2\u001B[0m finnhub_client \u001B[38;5;241m=\u001B[39m finnhub\u001B[38;5;241m.\u001B[39mClient(api_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcktca5hr01qvc3s73mtgcktca5hr01qvc3s73mu0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(finnhub_client\u001B[38;5;241m.\u001B[39mcompany_news(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAAPL\u001B[39m\u001B[38;5;124m'\u001B[39m, _from\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2020-06-01\u001B[39m\u001B[38;5;124m\"\u001B[39m, to\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2020-06-10\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'finnhub'"
     ]
    }
   ],
   "source": [
    "import finnhub\n",
    "finnhub_client = finnhub.Client(api_key=\"cktca5hr01qvc3s73mtgcktca5hr01qvc3s73mu0\")\n",
    "\n",
    "print(finnhub_client.company_news('AAPL', _from=\"2020-06-01\", to=\"2020-06-10\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:54:04.494079Z",
     "start_time": "2023-11-02T15:54:04.454056Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
